---
title: Evaluating the Importance of Initial Conditions for Antarctic Sea Ice Seasonal Predictability with a Fully Coupled Forecast Model
format:
  nature-pdf:
    # journal.cite-style is included in the tex file but ignored by pandoc if 
    # cite-method is not `natbib`.
    journal:
      cite-style: sn-mathphys-num
    # `citeproc` is the pandoc default. Set `cite-method: natbib` if required
    # to use the bst styles from the upstream template.
    cite-method: natbib
    keep-tex: true
    equal-margins: false
    classoption: referee,lineno
    fig-width: 5  
    fig-pos: "!htb"
header-includes: | 
   \usepackage[section]{placeins}
author:
  - name: Elio Campitelli
    affiliations:
      - name: Monash University, Australia,
        id: 1
        department: School of Earth, Atmosphere and Environment
        # address: 1737 Cambridge Street
        # city: Cambridge
        # state: MA
        # postal-code: 02138
      - name: Monash University, Australia
        id: 2
        department: Securing Antarctica’s Environmental Future
        # address: 1 Oxford Street
        # city: Cambridge
        # state: MA
        # postal-code: 02138
    email: elio.campitelli@monash.edu
    attributes:
      corresponding: true
  - name: Ariaan Purich
    affiliations:
      - ref: 1
      - ref: 2
  - name: Julie Arblaster
    affiliations:
      - ref: 1
      - ref: 2
  - name: Eun-Pa Lim
    affiliations:
      - department: Research, Bureau of Meteorology, Australia
        id: 3
  - name: Matthew C. Wheeler
    affiliations:
      - ref: 3      
  - name: Phillip Reid
    affiliations:
      - ref: 3      
keywords: [sea ice, seasonal predictability, initial conditions, forecasting]
bibliography: references.bib  

execute: 
  echo: false
  warning: false
  message: false
  cache: true

filters:
  - _extensions/ute/search-replace/search-replace.lua
  - _extensions/pandoc-ext/abstract-section/abstract-section.lua
  # - ../../_extensions/mloubout/critic-markup/critic-markup.lua
  
search-replace:
  +S2: ACCESS-S2
  +S1: ACCESS-S1
  +CDR: NOAA/NSIDC CDRV4

---

```{r setup, include=FALSE, cache = FALSE}
library(lubridate)
library(ggplot2)
library(data.table)
library(metR)
library(rcdo)
library(tagger)
library(patchwork)
library(mirai)

cdo_options_set("-L")
cdo_cache_set(here::here("data/temp/cache"))
```

```{r zenodo, cache = FALSE}
source(here::here("R/zenodo.R"))
zenodo_download_data()
zenodo_init()
```

```{r helpers, cache = FALSE}
source(here::here("R/functions.R"))
source(here::here("R/datasets.R"))
source(here::here("R/ggplot.R"))
```

```{r mirai_daemons, cache = FALSE}
if (!mirai::daemons_set()) {
  invisible(daemons(6))
}
```

# Abstract

Accurate Antarctic sea-ice forecasts are crucial for climate monitoring and operational planning, yet they remain challenging due to model biases and complex ice-ocean-atmosphere interactions. 
The two versions of the Australian Bureau of Meteorology's ACCESS seasonal forecast system, +S1 and +S2, use identical model configuration and differ only in their initial conditions; primarily in that +S2 does not assimilate sea-ice observations, whereas +S1 does.   
This provides an convenient opportunistic experiment to assess the role of initial conditions on Antarctic sea-ice forecasts using more than 20 years of fully coupled simulations with two 9-member ensembles.
Our analysis reveals that both systems experience an extended melt season and delayed growth phase compared with observations. 
This leads to a significant negative sea-ice extent bias, which is corrected only in +S1 by the data assimilation system. 
The impact of the differing initial conditions on forecast errors varies dramatically by season: summer and autumn initial conditions (January-April) provide predictive skill for up to three months, with February initial conditions being particularly crucial. 
In contrast, winter forecasts of the two systems are statistically indistinguishable after only two weeks. 
Regional analysis of forecast skill suggests that this winter predictability barrier is most dramatic over East Antarctica, where even +S1 shows negative skill. 
These findings highlight the critical importance of comprehensive year-round sampling in predictability studies and suggest that operational sea-ice data assimilation efforts should prioritize the summer-autumn period when initial conditions have maximum impact on forecast skill.

# Introduction

Accurately modelling Antarctic sea ice is essential for understanding processes and improving climate projections to inform adaptation strategies.
Accurate seasonal to sub-seasonal forecasts are also crucial for operation contingency planning in and around the Antarctic continent, including scientific missions, fisheries, and tourism [@desilva2020; @wagner2020].
Improvements in modelled sea-ice might also help improve weather forecasts over and away from sea-ice regions [@rinke2006; @wang2024; @semmler2016].

However, progress in Antarctic sea-ice forecasting system has lagged behind Arctic sea-ice forecasts due to model biases, and inherent large variability and complexity [@zampieri2019; @gao2024a].
Dynamical seasonal forecasts of summer Antarctic sea ice have been shown to perform worse than relatively simpler statistical methods [@massonnet2023] and machine learning approaches (e.g. @dong2024, @lin2025), which also underscores the need for better understanding and physical modelling of sea-ice dynamics, and drivers of its variability.

Good initial conditions are generally required for a good forecast, however, it is not entirely known to what extent accurate sea-ice initial conditions affect the quality of the forecast and at what timescales.
Exploring seasonal predictions of Arctic sea ice, @guemas2016 found that sea-ice initial conditions are important in autumn to predict summer sea ice, but the impact wasn’t as dramatic when predicting winter sea ice.
@day2014 also found seasonally-varying differences in the effect of initialisation, noting that accurate Arctic sea-ice thickness leads to improved sea-ice forecasts initialised in July but not when initialised in January.

For the Antarctic, @holland2013 studied the initial-value predictability of Antarctic sea ice in a perfect model study using the CCSM3 model.
They found that sea-ice and ocean initial conditions provide predictive information to forecast sea-ice edge location several months in advance and that some predictability is retained for up to two years thanks to ocean heat content anomalies that are advected eastward.
This is in contrast with @marchi2020, who ran perfect model experiments to argue that uncertainty in the predicted atmospheric state and evolution is the main driver of uncertainty in Antarctic sea-ice extent prediction on seasonal timescales, with sea-ice and ocean initial conditions having lesser importance.
More recently, @morioka2022 studied decadal forecasts of Antarctic sea ice and found that initialising ocean and sea ice improved the correlation between simulated and observed sea-ice concentration evolution in the Amundsen–Bellingshausen Sea.
It is hard to compare these studies since they are based on forecasts initialised at different times of the year and different frameworks: @holland2013 ran 20 ensemble members initialised on the 1st of January of a particular year, @marchi2020 ran forecasts from the 1st of March and 1st of September, and @morioka2022 ran forecasts only from the 1st of March.
@marchi2020 also used a coupled ocean–sea-ice model instead of a fully coupled model like @holland2013 did.
@morioka2022 used observed sea-ice initial conditions and compared with observations, while @marchi2020 and @holland2013 were perfect model studies.

In October 2021 the Australian Bureau of Meteorology (BoM) upgraded the Australian Community Climate and Earth System Simulator – Seasonal (ACCESS-S) from version S1 to S2.
While the base model remained the same, the change in version was focused on using ocean, sea-ice and land initial conditions generated by the BoM instead of depending on the UK Met Office.
Crucially, compared to +S1, +S2 does not assimilate sea-ice observations, so sea ice is only affected by the ocean and atmospheric data assimilation via the coupled integration.

Since model configuration is identical between +S1 and +S2, they form a sort of "opportunistic experiment” where the same forecasting model was run over a long period of time with multiple ensemble forecasts initialised throughout the year, with the only difference being the initial conditions.
This provides an opportunity to test the effect of sea-ice initial conditions on the forecast of sea-ice concentrations and the climate.

In this study we compare sea-ice hindcasts produced by +S1 and +S2.
We focus on seasonality of errors and biases and the effect of the data assimilation system.
This comparison will inform future work with the prediction system as a research tool to better understand the dynamics and variability of the Antarctic sea ice and its impacts on the climate system as well as to explore the potential of using its sea-ice forecasts for decision-making.
The work will also serve as a benchmark for future prediction systems to attempt to improve upon.

# Data and methods

## +S2

+S2 [@wedd2022] is the Bureau of Meteorology's seasonal forecast system which became operational in October 2021, replacing the +S1 system [@hudson2017].
The model components of both +S2 and +S1 are identical with the same numbers of levels and resolution.
They consist of the Global Atmosphere 6.0 (GA6) [@williams2015; @waters2017], the Unified Model's Global Land 6.0 [@best2011; @waters2017], NEMO Global Ocean 5.0 [@gurvan2013; @megann2014] and Global Sea Ice 6.0 [CICE; @rae2015].
The atmosphere has a N216 horizontal resolution (\~60 km in the mid-latitudes) with 85 vertical levels.
The land model uses the same horizontal grid as the atmosphere with four soil levels.
The ocean component has a nominal horizontal resolution of 1/4º with 75 vertical levels.
The sea-ice component, based on CICE version 4.1, has the same resolution as the ocean component and five sea-ice thickness categories as well as an open water category.

Both systems take atmospheric initial conditions derived from ERA-interim [@dee2011] for their hindcasts.
The main difference between the hindcasts of the two systems are the ocean and sea-ice initial conditions.
+S1's ocean and sea-ice initial conditions come from the Met Office FOAM system, which uses a multivariate, incremental three-dimensional variational (3D-Var), first-guess-at-appropriate-time (FGAT) data assimilation scheme [@waters2015] and assimilates sea surface temperature (SST), sea surface height (SSH), in situ temperature and salinity profiles, and satellite observations of sea-ice concentration using the EUMETSAT OSISAF product described in the next section.
+S2, on the other hand, is initialised from ocean conditions generated by the BoM weakly coupled ensemble data assimilation scheme described in @wedd2022.
This scheme uses an optimal interpolation method and assimilates temperature and salinity profiles from EN4 [@good2013].
SSTs are nudged to Reynolds OISSTv2.1 [@reynolds2007] in areas where SSTs are over 0ºC and Sea Surface Salinity is weakly nudged to the World Ocean Atlas 2013 climatology [@zweng2013].

Of most relevance for this work, sea-ice concentrations are not assimilated in +S2.
Assimilation cycles are performed daily.
The coupled model runs for 24 hours initialised from the previous cycle.
Then the restart file fields of the ocean component are used as first guess in the data assimilation cycle and the innovations are used to build the next ocean initial conditions for the following cycle.
The atmosphere fields from that daily integration are not used and instead the model atmosphere is initialised using ERA-Interim.
The sea-ice initial conditions for the next cycle are the unaltered output of the previous daily integration.
Then the cycle starts again and the coupled model runs for another 24 hours.
During this integration the sea-ice component is affected by the ocean innovations and the new atmosphere initial conditions via the coupler.

The +S1 hindcast set is made up of nine members created by perturbing the atmospheric fields only with a random field perturbation [@hudson2017] and runs for 217 days for the period 1990–2012 initialised at the first of every month.
The +S2 hindcast set used in this study runs for the period 1981–2018. Each hindcast consists of nine ensemble members built from three consecutive three-member forecasts created in the same manner as +S1 members initialised at the first of every month and the two previous days and run for 279 days. 
We analyse the ensemble mean hindcasts unless otherwise specified. 

Anomalies for each hindcast set are taken with respect to their own climatology specific to each initialisation date and forecast lead time, for the period 1990–2012.
This serves as a first-order correction of model bias and drift.
For monthly means, we define "0 lead time months" as the monthly mean forecast of the same month of initialisation. 

Besides sea-ice concentration, we also analyse mean sea-ice thickness, which we compute as total sea-ice volume divided by total sea-ice area. 


## Verification datasets

```{r datasets}
datasets <- list(
  cdr = CDR() |>
    zenodo("NSIDC CDR sea-ice concentration."),
  osi = OSI() |>
    zenodo("OSI sea-ice concentration.")
)
labs_dataset <- c(cdr = "CDR", osi = "OSI")
which_dataset <- "cdr"
```

For verification we use satellite-derived sea-ice concentration, which estimates the proportion of each grid area that is covered with ice.
Datasets derived using different algorithms and satellite platforms, each have their own biases and uncertainties.
Estimates of inter-product uncertainty of sea-ice extent (SIE, defined here as the total region of the Southern Ocean with at least 15% sea-ice cover) is of the order of 0.5 million $km^2$ [@meier2019].
As will be shown below, this spread is minimal compared with the typical errors in the +S2 and +S1 forecasts, so the overall conclusions of this study are independent of the verification dataset used.

We use NOAA/NSIDC's Climate Data Record V4 [CDR; @meier2014] as the primary sea-ice verification dataset.
It takes the maximum value of the NASA Team [@cavalieri1984] and NASA Bootstrap [@comiso2023] sea-ice concentration products to reduce their low concentration bias [@meier2014; @meier2021].
Both source algorithms use data from the Scanning Multichannel Microwave Radiometer (SMMR) on the Nimbus-7 satellite and from the Special Sensor Microwave/Imager (SSM/I) sensors on the Defense Meteorological Satellite Program's (DMSP) -F8, -F11, and -F13 satellites.
The data has a spatial resolution of 25 by 25 km and daily from November 1978 onwards.

The European Organisation for the Exploitation of Meteorological Satellites (EUMETSAT) Ocean and Sea Ice Satellite Application Facility [OSI; @OSISAF] based on the SSMIS sensor is another satellite-derived sea-ice concentration product. 
It is based on mostly the same sensors as the NOAA CDR but computed independently using different algorithms. 
Figures prepared with this dataset are provided in the supplementary material and do not differ significantly from the ones prepared using CDR.

## Error measures

For evaluation purposes, we use a series of measures. 
Sea-ice extent is defined as the area of the ocean at least 15% covered by sea-ice. 
This threshold is motivated by the limitations in satellite retrieval, which is increasingly unreliable for lower sea-ice concentrations [@cavalieri1991].

Pan-Antarctic (net) sea-ice extent serves as a hemispheric measure of the amount of sea ice, but it does not take into account the spatial distribution. 
A model could have a relatively accurate extent of the net ice but with different regional distributions. 
To account for location errors, we computed the Root Mean Squared Error (RMSE) of grid-point sea-ice concentration anomalies.

We compute RMSE as the square root of the area-averaged squared differences between grid-point forecasted and observed sea-ice concentration anomalies.
We compute a pan-Antarctic RMSE by averaging over the whole NOAA/NSIDC CDRV4 Southern Hemisphere domain, and also a zonally-varying RMSE computed over 15 longitude slices 24° wide around Antarctica.

All error measures were computed on the NOAA/NSIDC CDRV4 domain grid, to which model output was bilinearly interpolated. 
Note that the ACCESS CICE model grid has resolution between two and three times higher than NOAA/NSIDC CDRV4.

Forecasts errors are also compared with hypothetical forecasts based on the persistence of anomalies and on climatology. 
The persistence forecast is generated by extending the observed anomalies in time. 

As a measure of forecast improvement over the hypothetical forecast, we use the skill score [@murphy1985], defined as

$$
S = 1 - \frac{RMSE_{f}}{RMSE_{r}}
$$

Where $RMSE_{f}$ is the RMSE of the forecast, $RMSE_{r}$ is the RMSE of the reference forecast. 
Negative skill score indicates that the forecast is worse than the reference forecast while positive values indicate an improvement. 
A perfect forecast, would have zero RMSE and thus a skill score of 1. 


## Computational procedures 

We performed all analysis in this paper using the R programming language [@rcoreteam2020], using data.table [@dowle2020] and metR [@campitelli2020b] packages.
Significant processing was performed using the CDO command line operators [@schulzweida2023].
All graphics are made using ggplot2 [@wickham2009].
The paper was rendered using knitr and quarto [@xie2015; @allaire2022].

# Results and discussion

## Bias

```{r extent_daily}
extent_daily <- datasets |>
  lapply(extent) |>
  lapply(\(x) ReadNetCDF(x, "aice")) |>
  rbindlist(idcol = "dataset") |>
  _[, let(lon = NULL, lat = NULL)] |>
  _[, anom := aice - mean(aice, na.rm = TRUE), by = .(yday(time), dataset)] |>
  # some values before 2010 are zero or extremely low. likely due to missing values.
  _[year(time) < 2010 & anom < -2.5e12, aice := NA] |>
  _[, anom := NULL] |>
  _[, time := update(time, hour = 0)]
```

```{r hindcast_extent}
hindcast_extent <- here::here("data/derived/hindcast_extent.csv") |>
  zenodo("Sea-ice extent from hindcast.") |>
  fread() |>
  _[, model := relevel(factor(model), "S2")] |>
  _[, lag := as.numeric(as.Date(time) - as.Date(forecast_time))] |>
  _[lag > 0] |>
  _[!(month(forecast_time) == 1 & member == 7 & model == "S1")] # This member is wrong
```

```{r extent_climatology}
ranges <- hindcast_extent[, .(range = list(range(time))), by = model]

extent_climatology <- purrr::pmap(ranges, \(model, range) {
  extent_daily |>
    _[time %between% range] |>
    _[aice == 0, aice := NA] |>
    _[!(month(time) == 2 & mday(time) == 29)] |>
    _[, time := update(time, year = 2001)] |>
    _[, average(aice), by = .(time, dataset)] |>
    _[, model := model] |>
    _[]
}) |>
  rbindlist()
```

```{r range_max_lag}
range_max_lag <- hindcast_extent[
  mday(time) == 2,
  .SD[which.max(lag)],
  by = .(forecast_time, model)
] |>
  _[, .(max = max(lag), min = min(lag)), by = model]
```

```{r fig-hindcast-extent}
#| fig-cap: "Row a: Pan-Antarctic mean sea-ice extent for all hindcasts initialised on the first of each calendar month for +S1 (column 1; green) and +S1 (column 2; purple). Observed mean sea-ice extent in each corresponding hindcast period is shown in black. Row b: Mean differences between the forecast and the observed values. Circles represent the initial conditions at the start of forecasts (i.e., the first of every month), and triangles represent the mean values at the end of forecasts (i.e., the longest possible lead time)."
#| fig-height: 4

hindcast_extent[, .(model, forecast_time, member, time, Forecast = aice)] |>
  extent_daily[i = _, on = c("time"), allow.cartesian = TRUE] |>
  _[, Difference := Forecast - aice] |>
  _[, aice := NULL] |>
  tidyfast::dt_pivot_longer(cols = c(Forecast, Difference)) |>
  _[,
    average(value),
    by = .(
      dataset,
      model,
      measure = name,
      forecast_time = update(forecast_time, year = 2001),
      lag = as.numeric(as.Date(time) - forecast_time)
    )
  ] |>
  _[, time := forecast_time + lag] |>
  _[, time2 := update(time, year = 2001)] |>
  _[, month := lubridate::month(forecast_time, label = TRUE)] |>
  _[, model := factor(model, levels = c("S1", "S2"))] |>
  _[, measure := factor(measure, levels = c("Forecast", "Difference"))] |>
  _[dataset == which_dataset] |>
  ggplot(aes(time2, estimate)) +
  geom_line(
    data = extent_climatology[dataset == which_dataset] |>
      _[, measure := factor("Forecast", levels = c("Forecast", "Difference"))],
    aes(x = as.Date(time), colour = dataset, group = dataset)
  ) +

  geom_hline(
    data = data.table(
      y = 0,
      measure = factor("Difference", levels = c("Forecast", "Difference"))
    ),
    aes(yintercept = 0)
  ) +
  geom_line(aes(
    group = interaction(forecast_time, time != time2),
    colour = model
  )) +
  geom_point(
    data = \(x) {
      x[
        mday(time) == 2,
        .SD[which.max(lag)],
        by = .(forecast_time, model, measure)
      ]
    },
    aes(fill = model),
    shape = 24,
    size = 2.2
  ) +
  geom_point(data = \(x) x[lag == 1], aes(fill = model), shape = 21, size = 2) +

  scale_x_date(
    NULL,
    date_breaks = "2 month",
    date_labels = "%b",
    expand = c(0, 0)
  ) +
  scale_y_continuous(NULL, labels = labels_extent) +
  scale_color_models +
  scale_fill_models +
  guides(colour = "none", fill = "none") +
  # scale_color_manual(values = cetcolor::cet_pal(12, name = "c2"),
  #                    aesthetics = c("fill", "color"), guide = "none") +
  facet_grid(
    measure ~ model,
    scales = "free_y",
    labeller = labeller(model = labels_models)
  ) +
  tag_facets(tag = "rc") +
  coord_cartesian(clip = "off")
```

@fig-hindcast-extent shows mean sea-ice extent of the +S1 and +S2 hindcasts (row a) and their differences from mean sea-ice extent of +CDR (row b).
Mean extent at the first of every month is indicated with circles for the initial conditions and with triangles for the longest lead time possible for each model (between `r range_max_lag[model == "S2", min]` and `r range_max_lag[model == "S2", max]` days for +S2 and between `r range_max_lag[model == "S1", min]` and `r range_max_lag[model == "S1", max]` days for +S1).
At this long lead time, information of the initial conditions is essentially lost and the forecast reverts to each model's preferred equilibrium state.

+S2 initial conditions (circles in [Fig. @fig-hindcast-extent] column 2) show an overall negative bias, especially in the late summer-early autumn, while +S1 initial conditions (circles in [Fig. @fig-hindcast-extent] column 1) are very close to observations, as expected from the assimilation of sea-ice observations to produce the initial conditions of +S1.
Both systems' equilibrium states (triangles) show negative biases of sea-ice extent, particularly in the growth phase of late-autumn and winter months.
This is due primarily to the melt season being longer than in observations and with faster melt between January and March and the growing seasons being shorter with slower growth during March and April.
This is then followed by faster growth between May and July (@fig-mean-growth).
Many sea-ice models exhibit this systematic underestimation during the sea-ice minimum and early freezing season [@massonnet2023], which could indicate problems in the representation of thermodynamics in the model [@zampieri2019].
It is also not surprising that both forecasting systems converge to a similar equilibrium state because they share the same model formulation.

The difference between the initial conditions (circles) and the model equilibrium state (triangles) can be mostly attributed to the effect of data assimilation, which in +S2 is due solely to the coupling of sea-ice with the atmosphere and the ocean. 
From May to October, in +S2 circles are closer to observations than the triangles are, indicating that the information from the ocean and atmosphere data assimilation is affecting sea ice and improving the initial conditions.
During these months, +S1 can overestimate the sea-ice extent at short lead time.
For the rest of the year circles are overlaid with triangles in +S2, indicating that the ocean and atmosphere data assimilation is not affecting sea ice and that this component of the model is virtually free-running.

```{r extent-delta-compute}
n <- 366 * 3
w <- 11

dif_hindcast <- hindcast_extent |>
  copy() |>
  _[order(time)] |>
  _[, .(aice = mean(aice)), by = .(model, forecast_time, time)] |>
  _[,
    dif := c(NA, diff(aice) / diff(as.numeric(as.Date(time)))),
    by = .(model, forecast_time)
  ] |>
  _[,
    dif := frollmean(dif, n = w, align = "center"),
    by = .(model, forecast_time)
  ] |>
  na.omit() |>
  _[,
    average(dif),
    by = .(model, month(forecast_time), time = update(time, year = 2000))
  ]

dif_extent <- extent_daily |>
  _[order(time)] |>
  _[,
    dif := c(NA, diff(aice) / diff(as.numeric(as.Date(time)))),
    by = .(dataset)
  ] |>
  na.omit() |>
  _[, dif := frollmean(dif, n = w, align = "center"), by = .(dataset)] |>
  _[, average(dif), by = .(dataset, time = update(time, year = 2000))]

```

```{r fig-mean-growth}
#| fig-cap:  Mean daily sea-ice extent growth ($10^6 km^2/day$) in +S1 (green) and +S2 (purple) hindcasts and observations (black), computed as the mean daily differences in sea-ice extent between each date and the next for each forecast month. Values are smoothed with a 11-day running mean.
#| fig-height: 4

dif_extent |>
  _[dataset == which_dataset] |>
  ggplot(aes(time, estimate)) +
  geom_hline(yintercept = 0, color = "gray50", linewidth = 0.2) +
  geom_line(aes(color = dataset)) +
  geom_line(
    data = dif_hindcast,
    aes(color = model, group = interaction(model, month, month > month(time)))
  ) +
  scale_x_datetime(
    NULL,
    date_breaks = "1 month",
    date_labels = "%b",
    expand = c(0, 0)
  ) +
  scale_y_continuous(NULL, labels = \(x) {
    labels_extent(x, units = "M km²/day")
  }) +
  scale_color_models +
  coord_cartesian(xlim = as.POSIXct(c("2000-01-01", "2000-12-31"), tz = "UTC"))
```

```{r hindcast_clim}

hindcast_clim <- hindcast_0lag_clim() |>
  _[, ReadNetCDF(file, "aice"), by = .(model)] |>
  _[, month := month(time)] |>
  _[, time := NULL]

monnb <- function(d) {
  lt <- as.POSIXlt(as.Date(d, origin = "1900-01-01"))
  lt$year * 12 + lt$mon
}
mondf <- function(d1, d2) {
  monnb(d2) - monnb(d1)
}

obs_clim <- lapply(datasets, \(x) {
  x |>
    climatology() |>
    cdo_ymonmean() |>
    cdo_execute() |>
    ReadNetCDF(c(obs = "aice"))
}) |>
  rbindlist(idcol = "dataset") |>
  _[, month := month(time)]

hindcast_clim <- hindcast_clim |>
  merge(obs_clim)
```

```{r}
#| label: plot_bias
plot_bias <- lapply(c("S2", "S1"), \(x) {
  hindcast_clim |>
    _[model == x] |>
    _[dataset == which_dataset] |>
    # dcast(time + xgrid + ygrid ~ model, value.var = "aice") |>
    # setnames(which_dataset, "obs") |>
    ggplot(aes(xgrid, ygrid)) +
    geom_contour_fill(
      aes(z = aice - obs, fill = after_stat(level)),
      breaks = AnchorBreaks(binwidth = 0.1, exclude = 0)(c(-0.999, 0.999))
    ) +
    scale_fill_divergent_discretised(
      paste0("Bias with respect to ", labels_models[which_dataset]),
      low = trans_pink,
      high = trans_blue
    ) +
    # geom_polygon(data = ice_shelves_nsicd, aes(X, Y, group = group),
    #              colour = "black", fill = "#FAFAFA") +
    geomcoord_antarctica +
    geom_antarctica_fill +
    facet_wrap(~month, labeller = labeller(month = labels_month)) +
    wide_legend +
    tag_facets()
})

```

```{r fig-bias-1}
#| fig-cap: !expr glue::glue("Ensemble mean difference between monthly sea-ice concentration of +S2 ensemble mean forecast at 0-month lead time (monthly mean values forecasted from the forecast initialised at the first of the month) and observations ({labs_dataset[which_dataset]}).")
#| fig-height: 6

plot_bias[[1]]
```


```{r fig-bias-2}
#| fig-cap: Same as @fig-bias-1 but for +S1.
#| fig-height: 6

plot_bias[[2]]
```


To further understand the bias in +S2, @fig-bias-1 shows spatial patterns of the differences of monthly mean sea-ice concentrations between NOAA/NSIDC CDRV4 and +S2 hindcasts at the shortest monthly lead time.
From October to May, the model underestimates sea-ice concentrations in most regions except for the inner Weddell Sea in April and May, where sea-ice concentrations saturate to 1 both in the observations and forecasts.
In winter, the differences are limited to a narrow band around the sea-ice edge with slight positive biases in the African sector of East Antarctica and negative biases around the Indian Ocean sector which partially compensate, resulting in the near-zero extent bias seen in those months (@fig-hindcast-extent).

+S1 has a comparatively smaller overall bias (@fig-bias-2). 
The largest values are found between April and June, when the faster growth results in large positive bias along the sea-ice edge, and in January, when the faster melt leads to large negative bias in the Weddell and Admunsen Seas. 

\FloatBarrier

## RMSE

```{r fig-extent-anom}
#| fig-cap: Monthly mean sea-ice extent anomalies of the observations (black) and forecasts from +S1 (right column; purple) and +S2 (left column; green) at lead times of 0, 2, 4, and 6 months. The RMSE and correlation during the overlapping period of +S1 and +S2 hindcasts (1990–2013) are shown on the top left and bottom left of each panel respectively.
#| fig-height: 6

months_difference <- function(x, y) {
  lubridate::interval(y, x) %/% months(1)
}

monthly_extent <- extent_daily |>
  _[dataset == which_dataset] |>
  _[aice == 0, aice := NA] |>
  _[,
    .(aice = mean(aice)),
    by = .(dataset, time = as.Date(round_date(time, "month")))
  ] |>
  _[,
    aice := Anomaly(aice, year(time) %between% c(1981, 2011), na.rm = TRUE),
    by = .(month(time), dataset)
  ] |>
  _[]

N <- uniqueN(monthly_extent$time)

s1_range <- hindcast_extent[model == "S1"][, range(time)]
s2_range <- hindcast_extent[model == "S2"][, range(time)]

hindast_extent_monthly <- hindcast_extent |>
  copy() |>
  _[,
    aice := aice - mean(aice[year(time) %between% c(1981, 2011)]),
    by = .(lag, model, time = update(time, year = 2000))
  ] |>
  _[,
    .(forecast = mean(aice)),
    by = .(model, forecast_time, time = as.Date(round_date(time, "month")))
  ] |>
  _[, lag := months_difference(time, forecast_time)]

hindast_extent_monthly |>
  _[, merge(.SD, monthly_extent, all = TRUE), by = .(model, lag)] |>
  _[lag %in% (c(1, 3, 5, 7) - 1)] |>
  _[time %between% s2_range] |>
  ggplot(aes(time)) +
  geom_line(aes(y = aice, group = dataset, color = dataset), linewidth = 0.2) +
  geom_line(aes(y = forecast, color = model), linewidth = 0.2) +
  geom_text(
    data = \(x) {
      x[
        time %between% s1_range,
        rmse(aice, forecast, signif = 2, scale = 1e12),
        by = .(lag, model)
      ]
    },
    aes(label = paste0("RMSE: ", text)),
    x = -Inf,
    y = Inf,
    size = 2.5,
    vjust = 2.5,
    hjust = 0
  ) +

  geom_text(
    data = \(x) {
      x[time %between% s1_range, correlate(aice, forecast), by = .(lag, model)]
    },
    aes(label = paste0("cor: ", text)),
    vjust = -1.5,
    hjust = 0,
    size = 2.5,
    x = -Inf,
    y = -Inf
  ) +

  scale_y_continuous(NULL, labels = labels_extent) +
  scale_x_date(NULL, expand = c(0, 0)) +
  facet_grid(
    lag ~ model,
    labeller = labeller(lag = \(x) paste0("Month: ", x))
  ) +
  scale_color_models +
  scale_fill_models
```

@fig-extent-anom shows monthly sea-ice extent anomalies forecasted at selected lead times.
Compared with +S1, +S2 anomaly forecasts are relatively poor (large RMSE) even for the first month (lead time 0), whereas +S1 forecasts stay relatively skilful even at a lead time of three months.
+S2 shows much larger interannual variability than observations, with dramatic lows between 1995 and 2007, and highs between 2007 and 2015.

Unexpectedly, for +S2, RMSE improves with lead time, even though the correlation degrades with lead time.
This is puzzling behaviour that goes contrary to what is usually seen in prediction models.
The explanation seems to be the mentioned increased interannual variability.
@fig-extent-sd shows the interannual standard deviation of monthly sea-ice extent of the forecasts as a function of lead time compared with observations.
+S1 standard deviation lies within the observed standard deviation regardless of lead time, while +S2 standard deviation is more than twice that of observations at zero lead time and only approaches the observed value at nine month lead time for most months.


```{r fig-extent-sd}
#| fig-cap: Interannual standard deviation with 95% confidence interval of monthly mean sea-ice extent forecasted for each month divided by that month’s sea-ice extent observation standard deviation. +S1 and +S2 at different lead times. Each panel indicates the target month. Note the reverse horizontal axis.
#| fig-height: 5

obs_sd <- monthly_extent[
  time %between% s1_range,
  sd_ci(aice, na.rm = TRUE),
  by = .(month(time), dataset)
] |>
  _[, .(dataset, month, obs_sd = estimate)]

hindcast_extent_sd <- hindast_extent_monthly[
  time %between% s1_range,
  sd_ci(forecast, na.rm = TRUE),
  by = .(model, lag, month(time))
]

hindcast_extent_sd |>
  _[obs_sd, on = .NATURAL] |>
  _[dataset == which_dataset] |>
  ggplot(aes(lag, estimate / obs_sd)) +
  # geom_rect(data = obs_sd, inherit.aes = FALSE,
  #            aes(ymin = low, ymax = high,  fill = dataset, xmin = -Inf, xmax = Inf),
  #               alpha = 0.5) +
  # geom_hline(data = obs_sd, aes(yintercept = estimate, color = dataset)) +
  geom_ribbon(
    aes(ymin = low / obs_sd, ymax = high / obs_sd, fill = model),
    alpha = 0.5
  ) +
  geom_line(aes(color = model)) +

  scale_y_continuous("Standardised sea-ice extent standard deviation") +
  scale_x_continuous(
    "Lead time",
    breaks = c(0:9),
    expand = c(0, 0),
    trans = scales::reverse_trans()
  ) +
  scale_color_models +
  scale_fill_models +
  facet_wrap(~month, labeller = labeller(month = labels_month))

```

```{r forecast_example}
mlag <- 0
sample_date <- "2008-05-02"
forecast_date <- as.Date(sample_date)
month(forecast_date) <- month(sample_date) - mlag
mday(forecast_date) <- 1

aice_forecast_example <- c(
  lapply(c("S1", "S2"), \(model) {
    month_clim <- formatC(month(forecast_date), width = 2, flag = "0")
    clim <- here::here(glue::glue(
      "data/derived/climatology/{model}/{month_clim}/em.nc"
    )) |>
      zenodo("{model} sea-ice concentration climatology for {month_clim}")

    hindcast(forecast_date, model, 1) |>
      zenodo("{model} hindcast for initialised on {forecast_date}") |>
      cdo_ydaysub(clim) |>
      cdo_execute() |>
      ReadNetCDF("aice", subset = list(time = sample_date)) |>
      _[, model := model]
  }),
  list(
    CDR() |>
      anomalies() |>
      ReadNetCDF("aice", subset = list(time = sample_date)) |>
      _[, model := "cdr"]
  )
) |>
  rbindlist()

thickness_example <- c(
  S2 = hindcast(forecast_date, "S2", members = 1, variable = "hi") |>
    zenodo(
      "S2 sea-ice thickness hindcast for {forecast_date} (1st ensemble member)"
    ),
  S1 = hindcast(forecast_date, "S1", members = 1, variable = "hi") |>
    zenodo(
      "S1 sea-ice thickness hindcast for {forecast_date} (1st ensemble member)"
    )
) |>
  purrr::imap(\(f, model) {
    f |>
      glue::glue() |>
      cdo_selname("hi") |>
      remap_cdr() |>
      cdo_execute(cache = TRUE) |>
      ReadNetCDF("hi", subset = list(time = sample_date))
  }) |>
  rbindlist(idcol = "model")

```

+S2 forecasts of sea-ice extent anomalies seem to align moderately well with observations (leading to moderately high correlation) but their magnitude is overestimated (leading to large errors).
This could be caused by +S2 sea ice being much more sensitive to atmospheric and oceanic forcing, perhaps due to lower thickness. 

As an example, @fig-forecast-example shows sea-ice concentration anomalies (top row) and sea-ice thickness and the difference between the two models (bottom row) for `r format(as.Date(sample_date), "%e %B %Y")` initialised one day prior; being that close to initialisation date, these are very approximately the initial conditions. 
+S1 sea-ice concentrations anomalies are very close to observations as expected from the system assimilating these data. 
+S2 sea-ice concentration anomalies, which are not assimilated, are not as close, but the large-scale pattern is aligned with observations.
The system simulates large positive anomalies in the Weddell and Ross Seas and slight negative anomalies in the Amundsen and Bellingshausen Seas.
The fact that +S2 can simulate this pattern without assimilating sea-ice data suggests that atmospheric and oceanic forcing were the dominant drivers.
However, the magnitude of the sea-ice anomalies is too big. It is plausible that this is due to the thinner ice simulated by +S2 (bottom row). 


```{r fig-forecast-example}
#| fig.cap: +S1 and +S2 hindcasts for 2 May 2008 at one day lead time. Top row shows sea-ice concentration anomalies forecasted by each system and the observations. Bottom row shows forecasted sea-ice thickness and the difference between +S1 and +S2.
#| fig-height: 6

b <- seq(-0.5, 0.5, by = 0.1)
b <- b[b != 0]

aice_forecast_example |>
  ggplot(aes(xgrid, ygrid)) +
  geom_contour_fill(
    aes(z = aice, fill = ..level..),
    breaks = AnchorBreaks(binwidth = 0.1, exclude = 0)
  ) +
  scale_fill_divergent_discretised(
    "Sea-ice concentration anomaly",
    low = trans_pink,
    high = trans_blue
  ) +
  geom_antarctica_fill +
  geomcoord_antarctica +
  facet_wrap(~model, labeller = labeller(model = labels_models)) +
  wide_legend +

  thickness_example |>
    _[, model := factor(model, levels = c("S1", "S2", "Difference"))] |>
    ggplot(aes(xgrid, ygrid)) +
  geom_contour_fill(
    aes(z = hi, fill = ..level..),
    breaks = c(AnchorBreaks(exclude = 0, binwidth = 0.25)(c(0, 2)), Inf)
  ) +
  # as.discretised_scale(scale_fill_viridis_c)("Sea ice thickness",
  #                                            guide = guide_colorsteps(order = 9)) +
  scale_fill_divergent_discretised(
    "Sea-ice thickness",
    high = scales::muted("#5ab4ac"),
    guide = guide_colorsteps(order = 9)
  ) +
  ggnewscale::new_scale_fill() +

  geom_contour_fill(
    data = \(x) {
      x |>
        dcast(time + xgrid + ygrid ~ model, value.var = "hi") |>
        _[, model := factor("Difference", levels = c("S1", "S2", "Difference"))]
    },
    aes(z = S1 - S2, fill = ..level..),
    breaks = c(-Inf, b, Inf)
  ) +

  scale_fill_divergent_discretised(
    "Sea-ice thickness difference",
    guide = guide_colorsteps(order = 0)
  ) +
  geom_antarctica_fill +
  geomcoord_antarctica +
  wide_legend +
  facet_wrap(~model) +
  plot_layout(ncol = 1)
```


```{r hindcast_mean_thickness}
hindcast_mean_thickness <- hindcast_mean_thickness() |>
  _[,
    ReadNetCDF(mean_thickness, c(thickness = "hi")),
    by = .(model, forecast_time)
  ] |>
  _[, time := as.Date(update(time, day = 1))] |>
  _[, lag := months_difference(time, forecast_time)]
```

```{r}
#| label: fig-mean-thickness
#| fig-cap: Mean and 95% interval of monthly mean sea-ice thickness for +S1 and +S2 at different lead times. Each panel indicates the target month. Note the reverse horizontal axis.
#| fig-height: 5

hindcast_mean_thickness[
  time %between% s1_range,
  average(thickness),
  keyby = .(lag, model, month(time))
] |>
  ggplot(aes(lag, estimate)) +
  geom_ribbon(aes(ymin = low, ymax = high, fill = model), alpha = 0.5) +
  geom_line(aes(color = model)) +
  # geom_smooth(method = "lm", aes(color = model)) +
  scale_color_models +
  scale_fill_models +
  scale_y_continuous("Mean sea-ice thickness") +
  scale_x_continuous(
    "Lead time",
    breaks = c(0:9),
    expand = c(0, 0),
    trans = scales::reverse_trans()
  ) +
  facet_wrap(~month, labeller = labeller(month = labels_month))
```

Extending beyond the one case in @fig-forecast-example, @fig-mean-thickness shows monthly mean sea-ice thickness as a function of lead time for +S1 and +S2.
Supporting the idea that thinner ice is what causes the increased extent variability in +S2, this system  simulates thinner sea-ice compared to +S1 overall at almost all lead times and in all months except for summer at short lead times (Dec-Jan, 0-1 months; Feb-Mar, 0-2 months).
However, in both systems, forecasted sea-ice is thicker at shorter lead times and then decreases, particularly in the summer months.
If thinner ice were a sufficient cause of increased variability, then we would expect variability to increase with lead time in both forecasting systems.

The fact that +S1 and +S2 share the same model configuration and that the increased variability is more extreme at short lead times ([Fig. @fig-extent-sd]) suggests that the data assimilation procedure is partly responsible. 
It is possible that sea-ice in the +S2 system is left in an unbalanced state after assimilating atmospheric and oceanic data but not sea-ice data, leading to large responses that are amplified by the thin ice in the initial states which then subside at longer lead times when the model is balanced. 

To assess +S2 forecasts in more detail, we compute error measures for all hindcasts started on the 1st of every month.

```{r errors}
errors <- here::here("data/derived/rmse.Rds") |>
  zenodo("Hindcast RMSE of sea-ice concentration anomalies") |>
  readRDS() |>
  _[value < 0.01, value := NA] |>
  _[value > 3e13, value := NA] |>
  _[, time := as.Date(time)] |>
  _[, lag := as.numeric(time - time_forecast)] |>
  _[version == "persistence", member := "0em"] |>
  _[member == "0em"] |>
  _[lag > 0] |>
  _[!(month(time_forecast) == 1 & member == "07" & version == "S1")] # This member is wrong
```

```{r clim_std}
clim_std <- datasets |>
  lapply(\(x) {
    x |>
      anomalies(climatology(x)) |>
      cdo_sqr() |>
      cdo_fldmean() |>
      cdo_sqrt() |>
      cdo_execute() |>
      ReadNetCDF(c(climatology = "aice")) |>
      _[, let(lat = NULL, lon = NULL)]
  }) |>
  rbindlist(idcol = "obs_dataset") |>
  _[, time := as.Date(time)] |>
  _[climatology == 0, climatology := NA] |>
  _[climatology < 0.05, climatology := NA]
```

```{r clim_std2}
clim_std2 <- clim_std[errors, on = .NATURAL] |>
  _[, let(climatology = NULL, value = climatology)] |>
  na.omit() |>
  _[, version := "climatology"]


```

```{r t_test}
t_test <- function(x, y, ...) {
  test <- t.test(x, y, ...)

  list(
    low = test[["conf.int"]][1],
    high = test[["conf.int"]][2],
    estimate = -diff(test$estimate),
    p.value = test[["p.value"]]
  )
}

max_lag <- errors[version == "S1", max(lag)]

dif <- errors |>
  na.omit() |>
  # _[obs_dataset == which_dataset] |>
  _[measure == "rmse"] |>
  _[version != "persistence"] |>
  _[lag <= max_lag] |>
  dcast(
    obs_dataset + time + time_forecast + member + lag ~ version,
    value.var = "value"
  ) |>
  _[,
    t_test(na.omit(S1), na.omit(S2)),
    by = .(obs_dataset, lag, month(time_forecast))
  ] |>
  _[p.value > 0.01] |>
  _[, .SD[which.min(lag)], by = .(obs_dataset, month)] |>
  _[order(month), .(month, lag, obs_dataset)]

```

```{r fig-rmse}
#| fig-cap: Mean RMSE of sea-ice concentration anomalies as a function of forecast lead time for all forecasts initialised on the first of each month compared with a reference forecast of persistence of anomalies (black) and climatology (gray). Only the first 120 days are shown. In parenthersis, the shortest time at which +S1 and +S2 mean RMSE is not statistically different at the 99% confidence level.
#| fig-height: 5

month_from_lag <- function(day = 1, step = 2) {
  force(day)
  force(step)
  function(x) {
    unique(x[, .(lag, month)])[,
      time := make_date(month = month, day = 1) + lag
    ] |>
      _[
        mday(time) == day,
        .(lag, month, month2 = lubridate::month(time, label = TRUE))
      ] |>
      _[, .SD[((seq_len(.N) + 1) %% step) == 0], by = month]
  }
}

labels_month_significant <- setNames(
  paste0(month.abb, " (", dif[obs_dataset == which_dataset]$lag, ")"),
  1:12
)

errors2 <- clim_std2 |>
  rbind(errors) |>
  _[measure == "rmse"] |>
  _[, time2 := update(time, year = 2000)] |>
  # merge(clim_std) |>
  _[,
    average(value),
    by = .(lag, version, measure, month(time_forecast), obs_dataset)
  ]

errors2 |>
  _[obs_dataset == which_dataset] |>
  _[lag <= 120] |>
  ggplot(aes(lag, estimate)) +

  geom_vline(
    data = month_from_lag(1, 1),
    aes(xintercept = lag),
    colour = "gray50",
    linewidth = 0.1
  ) +

  geom_text(
    data = month_from_lag(15, 1),
    aes(y = Inf, label = month2),
    size = 2.5,
    vjust = 1
  ) +

  geom_text(
    data = month_from_lag(1, 1),
    aes(y = -Inf, label = lag),
    size = 2.5,
    vjust = 0
  ) +

  # geom_ribbon(aes(ymin = low,
  #                 ymax = high,
  #                 color = version,
  #                 fill = version,
  #                 group = interaction(obs_dataset, version)),
  #             alpha = 0.1) +
  geom_line(aes(color = version, group = interaction(obs_dataset, version))) +

  # geom_line(aes(y = clim_std)) +
  scale_x_continuous(breaks = NULL, expand = c(0, 0)) +
  labs(y = "RMSE", x = "Lead time (days)") +
  scale_color_models +
  scale_fill_models +
  facet_wrap(
    ~month,
    labeller = labeller(month = labels_month_significant),
    ncol = 3
  ) +
  coord_cartesian(clip = "off")
```

@fig-rmse shows the mean RMSE of sea-ice concentration anomalies for +S1 and +S2 hindcasts compared against persistence and climatological forecasts used as a benchmark.
Due to errors in the initial conditions, it is expected that persistence forecasts would be better than the model forecasts at very short lead times, but that the persistence forecast errors would grow faster and may eventually surpass the model forecast errors.
The black line shows that the persistence forecast error indeed grows rapidly and reaches its maximum in about 30 days for most months except for February, when it grows much slower.
The +S1 forecast errors grow slower than persistence forecast errors and remain lower after less than 10 days on average.
The +S2 forecast error starts high in all months and is lower than the persistence forecast error after more than 15 days in most months except for forecast initialised in February, when it takes 80 days.

At longer lead times, it is more appropriate to compare errors with the climatological forecast error. 
The lead time at which +S1 forecast error is higher than the climatological forecast error varies between more than 60 and less than 20 days depending on forecast initialisation month with the minimum in June.
+S2 forecasts never have lower error than climatology, on the other hand, except marginally in October forecasts. 

@fig-lead-time-window summarises the lead time window in which each hindcast is better than both the persistence forecast and the climatological forecast as a function of forecast month. 
+S1 forecasts have a wider lead time window in the summer than the other seasons and is not better than both benchmarks at forecasting June sea-ice concentration anomalies. 
Forecasts initialised in May and June are particularly poor, and July cannot be forecasted better than the benchmarks. 
This is consistent with the mid-winter loss of predictability observed by @libera2022, who attributed it to deep warm water entraining into the mixed layer. 



```{r fig-lead-time-window}
#| fig-cap: Minimum lead time at which each forecast's mean RMSE becomes larger than the lower bound of the 95% confidence interval of persistence forecast RMSE (black lines) and maximum lead time at which each forecast's mean RMSE remains lower than the lower bound of the 95% confidence interval of climatological forecast RMSE (gray lines). Green shading indicates the window where forecasts outperform both persistence (lead times longer than black line) and climatology (lead times shorter than gray line). Text labels show the date corresponding to the maximum lead time at which each forecast outperforms climatology.
#| fig-height: 4

rbind(
  errors2 |>
    copy() |>
    _[obs_dataset == which_dataset] |>
    _[,
      base := low[version == "persistence"],
      by = .(lag, measure, month, obs_dataset)
    ] |>
    _[version != "persistence"] |>
    _[version != "climatology"] |>
    _[estimate < base] |>
    _[, .SD[which.min(lag)], by = .(version, measure, month, obs_dataset)] |>
    _[, .(version, month, obs_dataset, lag, benchmark = "persistence")],

  errors2 |>
    copy() |>
    _[obs_dataset == which_dataset] |>
    _[,
      base := low[version == "climatology"],
      by = .(lag, measure, month, obs_dataset)
    ] |>
    _[version != "climatology"] |>
    _[version != "persistence"] |>
    # _[lag > 2] |>
    _[, rle(estimate < base), by = .(version, measure, month, obs_dataset)] |>
    _[, .SD[1], by = .(version, measure, month, obs_dataset)] |>
    tidyr::complete(
      version,
      measure,
      month,
      obs_dataset,
      values,
      fill = list(lengths = 0)
    ) |>
    as.data.table() |>
    _[values == TRUE] |>
    _[, .(
      version,
      month,
      obs_dataset,
      lag = lengths,
      benchmark = "climatology"
    )]
) |>
  _[, time := make_date(2000, month, 1) + lag] |>
  # _[benchmark == "climatology" & version == "S1"] |>
  ggplot(aes(month, lag)) +
  ggbraid::geom_braid(
    data = \(x) dcast(x, month + lag + version ~ benchmark, value.var = "lag"),
    alpha = 0.3,
    aes(
      ymin = persistence,
      ymax = climatology,
      fill = climatology > persistence
    )
  ) +
  geom_line(aes(color = benchmark)) +
  # geom_point(aes(color = time), data = \(x) x[lag != 0]) +
  ggrepel::geom_text_repel(
    data = \(x) {
      x[, .SD[which.max(lag)], by = .(version, month, obs_dataset)] |>
        _[benchmark == "climatology"]
    },
    aes(label = format(time, "%d %b")),
    size = 2,
    vjust = -0.4
  ) +
  geom_point(
    data = \(x) {
      x[, .SD[which.max(lag)], by = .(version, month, obs_dataset)] |>
        _[benchmark == "climatology"]
    },
    aes(color = benchmark)
  ) +
  scale_color_models +
  scale_y_continuous("Lead time (days)") +
  scale_x_continuous(
    "Forecast month",
    breaks = 1:12,
    labels = month.abb,
    minor_breaks = NULL
  ) +
  scale_fill_manual(
    guide = "none",
    values = c(`TRUE` = "seagreen", `FALSE` = "#FFFFFF00")
  ) +
  facet_wrap(~version, ncol = 1, labeller = labeller(version = labels_models)) +
  coord_cartesian(clip = "off") +
  tag_facets()

```

```{r topo_lonlat}
topo_lonlat <- here::here("data/raw/ETOPO.nc") |>
  zenodo("ETOPO topography") |>
  ReadNetCDF() |>
  na.omit()
```

```{r clim_std_lon} 
clim_std_lon <- datasets |>
  lapply(\(x) {
    x |>
      anomalies(climatology(x)) |>
      cdo_sqr() |>
      cdo_remapmean("r15x360") |>
      cdo_mermean() |>
      cdo_sqrt() |>
      cdo_execute() |>
      ReadNetCDF(c(climatology = "aice")) |>
      _[, let(lat = NULL, lon = lon + 12)]
  }) |>
  rbindlist(idcol = "obs_dataset") |>
  _[, time := as.Date(time)] |>
  _[climatology == 0, climatology := NA] |>
  _[climatology < 0.05, climatology := NA]

```

```{r rmse_lon_mean}
rmse_lon_mean <- here::here("data/derived/rmse_lon.Rds") |>
  zenodo("Hindcast RMSE in latitude slices") |>
  readRDS() |>
  _[version == "persistence", member := "0em"] |>
  _[member == "0em"] |>
  _[, lon := lon + 12] |>
  _[, time := update(time, hour = 0)] |>
  _[, time := as.Date(time)]
```


```{r rmse_lon-gdata}

rmse_lon_mean_gdata <- rmse_lon_mean |>
  _[version != "persistence"] |>
  _[, lag := as.numeric(as.Date(time) - time_forecast)] |>
  clim_std_lon[i = _, on = .NATURAL] |>
  _[,
    .(
      dif = mean(value - climatology, na.rm = TRUE),
      skill = 1 - (mean(value, na.rm = TRUE) / mean(climatology, na.rm = TRUE))
    ),
    by = .(lag, lon, version, obs_dataset, month = factor(month(time_forecast)))
  ]
```

```{r, fig.height=7, fig.width=10}
#| label: rmse_lon_plots

rmse_lon_mean_gdata2 <- rmse_lon_mean_gdata[obs_dataset == which_dataset]

range <- range(rmse_lon_mean_gdata2$skill)
binwidth <- 0.1
breaks <- AnchorBreaks(0, binwidth = binwidth)(range)
rmse_lon_plots <- lapply(c("S1", "S2"), \(x) {
  rmse_lon_mean_gdata2 |>
    _[version == x] |>
    # _[,
    #   skill := Smooth2D(lag, lon, skill, method = smooth_svd(0.05)),
    #   by = .(month, version)
    # ] |>
    ggplot(aes(lon, lag)) +
    geom_contour_fill(
      aes(z = skill, fill = after_stat(level)),
      breaks = breaks
    ) +
    geom_contour_tanaka(
      aes(z = skill),
      range = c(0.005, 0.05),
      breaks = breaks
    ) +

    geom_contour_fill(
      data = topo_lonlat,
      aes(lon, scales::rescale(lat, c(-60, 1)), z = z, fill = NULL),
      fill = "#FAFAFA",
      breaks = c(0, Inf)
    ) +

    geom_contour2(
      data = topo_lonlat,
      aes(lon, scales::rescale(lat, c(-60, 1)), z = z),
      linewidth = 0.2,
      breaks = 0
    ) +

    geom_segment(
      data = month_from_lag(1, 1),
      aes(y = lag, yend = lag, x = 7.5, xend = 360 - 7.5),
      colour = "gray50",
      alpha = 0.5,
      linewidth = 0.2
    ) +

    geom_text(
      data = month_from_lag(15, 1),
      aes(x = 360, label = month2),
      size = 1.5,
      hjust = 0.2
    ) +

    geom_text(
      data = month_from_lag(1, 1),
      aes(x = 0, label = lag),
      size = 1.5,
      hjust = 0.8
    ) +
    scale_fill_divergent_discretised(low = trans_blue, high = trans_pink) +
    scale_x_longitude(ticks = 90, expand = c(0.08, 0)) +

    scale_y_continuous(breaks = NULL, expand = c(0, 0)) +
    labs(fill = NULL, y = "Lead time (days)") +
    wide_legend +
    facet_wrap(~month, labeller = labeller(month = labels_month), ncol = 3) +
    theme(
      panel.background = element_blank(),
      panel.grid = element_blank(),
      tagger.panel.tag.background = element_rect(
        colour = "white",
        fill = "white"
      )
    ) +
    tag_facets(position = list(x = 0.08, y = 0, hjust = 0, vjust = 0))
})

```

```{r}
#| label: fig-rmse_lon-1
#| fig-cap: !expr glue::glue("RMSE skill score of +S1 forecasts with climatological forecast as reference computed on {uniqueN(rmse_lon_mean$lon)} meridional slices {diff(unique(rmse_lon_mean$lon))[1]}º wide as a function of lead time and longitude. Antarctica’s coastline is shown at the bottom of each panel for reference.")
#| fig-height: 7
rmse_lon_plots[[1]]
```


```{r}
#| label: fig-rmse_lon-2
#| fig-cap: Same as @fig-rmse_lon-1 but for +S2.
#| fig-height: 7
rmse_lon_plots[[2]]
```

To analyse the spatial distribution of the model error, we computed the RMSE of zonal mean sea-ice concentration anomalies on `r uniqueN(rmse_lon_mean$lon)` slices of `r diff(unique(rmse_lon_mean$lon))[1]`° longitude span for each forecasting system.
We control for some areas being naturally easier to forecast than others by computing the RMSE skill score with the climatological forecast RMSE as reference. 

For +S1 forecasts (@fig-rmse_lon-1), skill tends to be lower off the coast of Eastern Antarctica even at short lead times; for instance, the skill score for forecasts initialised in May and June are negative between 0° and 120°E even at almost zero lead time. 
This mirrors @libera2022 findings of a “winter predictability barrier”, although they focus on the Weddell Sea and here we show that the effect seems to be stronger more to the east. 
In West Antarctica there is a hint of easterly-propagating skill in forecasts initialised in February and March. 
This is consistent with @holland2013 findings that memory of sea-ice anomalies are stored in ocean heat content anomalies that are transported east by the Antarctic Circumpolar Current. 

+S2 forecasts  (@fig-rmse_lon-2) also have lower skill over East Antarctica. 
From July to December even though the pan-Antarctic average skill is negative at all lead times ([Fig. @fig-lead-time-window]), it is positive for up to a month in West Antarctica. 
Since oceanic and atmospheric forcing is the only source of information, this suggests that sea-ice in this region is particularly sensitive to oceanic and atmospheric forcing and suggests a role of the Pacific-South American mode and the Amundsen Sea Low to shape sea-ice concentration anomalies. 
The fact that this is evident in the months in which El Niño--Southern Oscillation teleconnections are more important for atmospheric circulation also suggest the influence of tropical Pacific variability.
February and March are the only two months that can be forecasted with marginally positive skill in large regions. 

```{r errors_ensemble} 
errors_ensemble <- here::here("data/derived/rmse.Rds") |>
  zenodo() |>
  readRDS() |>
  _[version != "persistence"] |>
  _[member != "0em"] |>
  _[value < 0.01, value := NA] |>
  _[value > 3e13, value := NA] |>
  _[!is.na(value)] |>
  _[, time := as.Date(time)] |>
  _[, lag := as.numeric(time - time_forecast)] |>
  _[lag > 0] |>
  _[!(month(time_forecast) == 1 & member == "07" & version == "S1")]

``` 


```{r fig-initial-spread}
#| fig-cap: Decomposition of forecast error spread at 1, 5 and 30 days lead time for +S1 and +S2 hindcasts across initialization months. The left column shows the mean standard deviation of RMSE errors across ensemble members, while the right column shows the standard deviation of the ensemble mean RMSE error and the spread of the persistence and climatology forecasts errors.
#| fig-height: 4
which_lags <- c(1, 3, 30)
rbind(
  mean_variance = errors_ensemble |>
    _[lag %in% which_lags] |>
    _[measure == "rmse"] |>
    _[obs_dataset == which_dataset] |>
    _[, var(value, na.rm = TRUE), by = .(version, lag, time_forecast)] |>
    _[,
      .(value = mean(V1, na.rm = TRUE)),
      by = .(version, month(time_forecast), lag)
    ],

  variance_of_mean = clim_std2 |>
    rbind(errors) |>
    _[measure == "rmse"] |>
    _[lag %in% which_lags] |>
    _[measure == "rmse"] |>
    # _[version != "persistence"] |>
    _[obs_dataset == which_dataset] |>
    _[,
      .(value = var(value, na.rm = TRUE)),
      by = .(version, month(time_forecast), lag)
    ],
  idcol = "component"
) |>
  ggplot(aes(month, sqrt(value))) +
  geom_line(aes(color = version), linewidth = 0.5) +
  scale_y_continuous("Standard deviation") +
  # scale_color_models +
  scale_x_continuous("Month", breaks = seq(1, 12, by = 2), labels = \(x) {
    month.abb[x]
  }) +
  scale_color_models +
  facet_grid(
    lag ~ component,
    labeller = labeller(
      component = c(
        mean_variance = "Mean standard deviation\nof errors in ensemble members",
        variance_of_mean = "Standard deviation\nof ensemble mean error."
      )
    )
  ) +
  tag_facets(tag = "rc")

```


In @fig-rmse the mean error was shown. 
@fig-initial-spread column 1 shows the mean standard deviation of errors among ensemble members at various lead times.
At one day lead time ([Fig. @fig-initial-spread] a.1) +S2 has a slightly larger spread than +S1 due to the way that ensemble members are generated.
+S1 ensemble members are generated by adding random field perturbations to the atmosphere only, which then are transferred to the other components via the coupled simulation [@hudson2017]. 
With this scheme, ensemble members are all but guaranteed to be underdispersed in the ocean and sea-ice components. The time-lag ensemble used for +S2 ensures greater spread. 
This difference is gone after about just two days, and both systems have a comparable spread in ensemble member error afterwards ([Fig. @fig-initial-spread] b1 and c1).

@fig-initial-spread column 2, on the other hand, shows the standard deviation of ensemble mean error of each hindcast and the persistence forecast. 
At one day lead time, +S2 ensemble mean error standard deviation is much larger than +S1’s, which in turn is comparable to the persistence forecast error standard deviation. 
At longer lead times, the spread of +S1 and persistence forecast standard deviation increases to eventually be comparable to +S2 and the standard deviation in climatological forecast errors. 
+S2 error standard deviation is fairly independent of lead time and similar to the climatological forecast error standard deviation at all lead times. 

\FloatBarrier

## Conclusions

Sea-ice forecasts from the +S2 system show a significant low extent bias, particularly during late summer and early autumn. 
This bias is attributed to a faster and longer melt season between January and March, and slower growth between March and April. 
This underestimation during the minimum and early freezing season is a common issue in many seasonal-to-subseasonal (S2S) systems, suggesting potential problems either with the model’s thermodynamic representation or with short wave radiation forcing, as shown in other climate models [@zampieri2019; @roach2020].
Even though +S2 shares the same model components as +S1, the latter does not suffer from this bias, indicating that assimilating sea-ice concentrations successfully corrects for the negative bias that exists in the free-running model.

Ensemble spread grows quickly even when perturbations are only implemented in the atmosphere component (in +S1), indicating that sea ice is indeed responding quickly to atmospheric perturbations. 
However, our analysis suggests that the atmosphere and ocean data assimilation implemented in +S2 is only effectively influencing sea-ice initial conditions from June to October, while the rest of the year, the sea-ice component runs virtually free, reverting to its biased equilibrium state. 

Analysis of the error spread shows that +S2 initial conditions from December to May not only have large errors, but that the initial error spread is very large compared with +S1.
This spread is not due to the perturbation scheme, since the mean error variance for individual forecasts is low and comparable with +S1.
Instead, it is due to large variance of the mean error of individual forecasts, which is comparable to the climatology spread. This is further evidence that individual initial conditions are not being affected by the data assimilation scheme.

Although +S1 only assimilates sea-ice concentration, it is clear that sea-ice thickness is also affected through the assimilation process. 
+S1 simulates significantly thicker ice than +S2 and in both systems sea-ice is thicker at shorter lead times than at longer lead times. 
Both the explicit data assimilation in +S2 and the effects of atmospheric and oceanic data assimilation in +S1 might be nudging simulated sea ice to be thicker than the model equilimbrium state.
We suggest that the thinner sea ice in +S2 contributes to the large sea-ice extent variance, but other mechanisms, such as unbalanced initial conditions might also be important. 

Given that +S2 sea-ice extent is not directly initialised by sea-ice observations, comparing its forecasts with those of +S1 allows us to estimate the time-scale over which initial conditions are important. 
We find that initial conditions affect Antarctic sea-ice forecasts in the order of a few months, but that effect is seasonally dependent. 
January to April initial conditions improve forecasts for up to three months with, February initial conditions in particular are shown to be crucial for determining sea-ice evolution at least up to May. 
Arctic sea-ice forecasts also show greater sensitivity to initial conditions in boreal summer, compared with boreal winter [@day2014; @bunzel2016], suggesting a similar mechanism might be playing a role. 

Forecasts initialised in winter have very little skill and +S1 and +S2 forecast errors are not statistically different after just two weeks. 
This is consistent with the findings of @libera2022's finding of a “winter predictability barrier” in the Weddell Sea, although they describe the barrier as sharp loss of predictability in July, and here we find a gradual reduction in skill compared with climatology around June. 
This difference might be due to our use of pan-Antarctic RMSE, since our regional analysis indicates that the degraded skill is most dramatic in the King Haakon sea. 

These findings have important implications for both operational forecasting, model development and predictability studies.
For operational centers, our results suggest that efforts to improve sea-ice data assimilation should prioritize the summer and autumn months when initial conditions have the greatest impact on forecast skill. 
Additionally, the substantial bias in +S2 highlights the need for improved model physics, particularly in the representation of sea-ice thermodynamics and radiation processes. 
Crucially, our results suggest dramatic seasonal variations in sea-ice predictability. 
Future studies should therefore use initial conditions through the whole year rather than focusing on a limited number of initialisation dates. 

# Acknowledgments

This work was supported by ARC SRIEAS Grant SR200100005 Securing Antarctica’s Environmental Future.


# Code and data availability 

The underlying code for this study is available on github and can be accessed via this link https://github.com/eliocamp/access-s2_ice-eval.

Raw data of +S1 and +S2 forecast are not available due to size. 
Derived datasets required to reproduce the results are available in this zenodo repository: XXXXX

# References

::: {#refs}
:::

\clearpage

\appendix  
\renewcommand{\thefigure}{A\arabic{figure}}
\setcounter{figure}{0} 

# Supplementary figures {.appendix}

The following are the same figures from the main paper but using the OSI dataset instead of CDR.

```{r dataset_era}
which_dataset <- "osi"
```

```{r fig-hindcast-extent-osi, ref.label=I('fig-hindcast-extent')}

```


```{r, fig-mean-growth-osi, ref.label=I("fig-mean-growth")}

```

```{r, plot_bias-osi, ref.label=I("plot_bias")}

```

```{r, fig-bias-1-osi, ref.label=I("fig-bias-1")}

```


```{r, fig-bias-2-osi, ref.label=I("fig-bias-2")}

```


```{r, fig-extent-anom-osi, ref.label=I("fig-extent-anom")}

```


```{r, fig-extent-sd-osi, ref.label=I("fig-extent-sd")}
```


```{r, fig-rmse-osi, ref.label=I("fig-rmse")}

```

```{r, fig-lead-time-window-osi, ref.label=I("fig-lead-time-window")}

```


```{r, rmse_lon_plots-osi, ref.label=I("rmse_lon_plots")}

```


```{r, fig-rmse_lon-1-osi, ref.label=I("fig-rmse_lon-1")}

```


```{r, fig-rmse_lon-2-osi, ref.label=I("fig-rmse_lon-2")}

```


```{r, fig-initial-spread-osi, ref.label=I("fig-initial-spread")}

```
