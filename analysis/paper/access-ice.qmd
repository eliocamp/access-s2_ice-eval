---
title: The Importance of Initial Conditions in Seasonal Predictions of Antarctic Sea Ice
pdf-engine: pdflatex
journal:
  name: bg
author:
  - given_name: Elio 
    surname: Campitelli
    affiliation: "1, 2"
    email: elio.campitelli@monash.edu
    corresponding: true
  - given_name: Ariaan
    surname: Purich
    affiliation: "1, 2"
  - given_name: Julie
    surname: Arblaster
    affiliation: "1, 2"
  - given_name: Eun-Pa
    surname: Lim
    affiliation: "3"
  - given_name: Matthew C.
    surname: Wheeler
    affiliation: "3"
  - given_name: Phillip
    surname: Reid
    affiliation: "3"
            
affiliation:
  - code: 1
    address: School of Earth, Atmosphere and Environment, Monash University, Kulin Nations, Clayton, Victoria, Australia. 
  - code: 2
    address: ARC Special Research Initiative for Securing Antarctica’s Environmental Future, Clayton, Kulin Nations, Victoria, Australia
  - code: 3
    address: Research, Bureau of Meteorology, Melbourne, Australia.

bibliography: references.bib  
running:
  title: The Importance of Initial Conditions in Seasonal Predictions of Antarctic Sea Ice
  author: Campitelli et al.
# This section is mandatory even if you declare that no competing interests are present.
competinginterests: |
  # Competing interests
  The authors declare no competing interests.
# See https://publications.copernicus.org/for_authors/licence_and_copyright.html, normally used for transferring the copyright, if needed. 
# Note: additional copyright statements for affiliated software or data need to be placed in the data availability section. 
# copyrightstatement: |
#   The author's copyright for this publication is transferred to institution/company. 
### The following commands are for the statements about the availability of data sets and/or software code corresponding to the manuscript.
### It is strongly recommended to make use of these sections in case data sets and/or software code have been part of your research the article is based on.
### Note: unless stated otherwise, software and data affiliated with the manuscript are assumed to be published under the same licence as the article (currently Creative Commons 4.0)
availability:
  #code: |
  #  use this to add a statement when having only software code available
  #data: |
  #  use this to add a statement when having only data sets available
  codedata: |
    # Code/Data availability 
    The underlying code for this study is available on GitHub: https://github.com/eliocamp/access-s2_ice-eval.
    Raw data of +S1 and +S2 hindcast are not available due to size. 
    Derived datasets required to reproduce the results, including extent timeseries and error measures, are available in this Zenodo repository: https://zenodo.org/records/17479538 [@campitelli2025]
authorcontribution: |
    # Author contributions
    EC performed the data analysis and wrote the manuscript draft. 
    AP, JA, EL, MW and PR, performed interpretation of the results, and reviewed and edited the draft. 
    All authors read and approved the final manuscript.

acknowledgements: |
    # Acknowledgements
    We thank the internal reviewers Bethan White and Xiaobing Zhou for their comments and feedback. 
    This work benefited from earlier unpublished work by Laura Davies, Phil Reid, Andrew G. Marshall. 
    This research was undertaken with the assistance of resources from the National Computational Infrastructure (NCI Australia), an NCRIS enabled capability supported by the Australian Government.
    This work was supported by ARC SRIEAS Grant SR200100005 Securing Antarctica’s Environmental Future.
# appendix: |
#   \section{Figures and tables in appendices}
#   Regarding figures and tables in appendices, the following two options are possible depending on your general handling of figures and tables in the manuscript environment:
#   \subsection{Option 1}
#   If you sorted all figures and tables into the sections of the text, please also sort the appendix figures and appendix tables into the respective appendix sections.
#   They will be correctly named automatically.
#   \subsection{Option 2}
#   If you put all figures after the reference list, please insert appendix tables and figures after the normal tables and figures.
  
#   To rename them correctly to A1, A2, etc., please add the following commands in front of them:
#   `\appendixfigures` needs to be added in front of appendix figures
#   `\appendixtables` needs to be added in front of appendix tables
  
#   Please add `\clearpage` between each table and/or figure.
format: 
  copernicus-pdf: 
      keep-tex: true
      classoption: referee,lineno
      fig-width: 6.5
      fig-pos: "!htb"
      fig-dpi: 300    
      fig-format: png
header-includes: | 
   \usepackage[section]{placeins}    

keywords: [sea ice, seasonal predictability, initial conditions, forecasting]


execute: 
  echo: false
  warning: false
  message: false
  cache: true


filters:
  - _extensions/ute/search-replace/search-replace.lua
  - _extensions/pandoc-ext/abstract-section/abstract-section.lua  


search-replace:
  +S2: ACCESS-S2
  +S1: ACCESS-S1
  +CDR: NOAA/NSIDC CDRV4

---

```{r setup, include=FALSE, cache = FALSE}
library(lubridate)
library(ggplot2)
library(data.table)
library(metR)
library(rcdo)
library(tagger)
library(patchwork)
library(mirai)

library(dtparallel)
parallel_method_set("mirai")

cdo_options_set("-L")
cdo_cache_set(here::here("data/temp/cache"))


knitr::opts_hooks$set(fig.cap = function(options) {
  if (is.null(options$fig.cap)) {
    options$fig.cap <- "CAPTION MISSING"
  }

  options$fig.cap <- glue::glue(options$fig.cap)

  return(options)
})
```

```{r zenodo, cache = FALSE}
source(here::here("R/zenodo.R"))
zenodo_download_data()
zenodo_init()
```

```{r helpers, cache = FALSE}
source(here::here("R/functions.R"))
source(here::here("R/datasets.R"))
source(here::here("R/ggplot.R"))
```

```{r mirai_daemons, cache = FALSE}
if (!mirai::daemons_set()) {
  invisible(daemons(6))
}

everywhere({
  library(data.table)
  rcdo::cdo_options_set("-L")
  rcdo::cdo_cache_set(here::here("data/temp/cache"))
})
```


# Abstract

Accurate Antarctic sea-ice forecasts are crucial for climate monitoring and operational planning, yet they remain challenging due to model biases and complex ice-ocean-atmosphere interactions. 
The two versions of the Australian Bureau of Meteorology's ACCESS seasonal forecast system, +S1 and +S2, use identical model configuration and differ only in their initial conditions; primarily in that +S2 does not assimilate sea-ice observations, whereas +S1 does.   
This provides a convenient opportunistic experiment to assess the role of initial conditions on Antarctic sea-ice forecasts using more than 20 years of fully coupled simulations with two 9-member ensembles.
Our analysis reveals that both systems experience an extended melt season and delayed growth phase compared with observations. 
This leads to a significant negative sea-ice extent bias, which is corrected only in +S1 by the data assimilation system. 
The impact of the differing initial conditions on forecast errors varies dramatically by season: summer and autumn initial conditions (January-April) provide predictive skill for up to three months, with February initial conditions being particularly crucial. 
In contrast, winter forecasts of the two systems are statistically indistinguishable after only two weeks. 
Regional analysis of forecast skill suggests that this winter predictive skill barrier is most dramatic over East Antarctica, where even +S1 shows negative skill. 
These findings highlight the critical importance of comprehensive year-round sampling in predictability studies and suggest that operational sea-ice data assimilation efforts should prioritise the summer-autumn period when initial conditions have maximum impact on forecast skill.

# Introduction

Accurately modelling Antarctic sea ice is essential for understanding processes and improving climate projections to inform adaptation strategies.
Accurate seasonal to sub-seasonal forecasts are also crucial for operation contingency planning in and around the Antarctic continent, including scientific missions, fisheries, and tourism [@desilva2020; @wagner2020].
Improvements in modelled sea-ice might also help improve weather forecasts over and away from sea-ice regions [@rinke2006; @wang2024; @semmler2016].

However, progress in Antarctic sea-ice forecasting system has lagged behind Arctic sea-ice forecasts due to model biases, and inherent large variability and complexity [@zampieri2019; @gao2024a].
Dynamical seasonal forecasts of summer Antarctic sea ice have been shown to perform worse than relatively simpler statistical methods [@massonnet2023] and machine learning approaches (e.g. @dong2024, @lin2025), which also underscores the need for better understanding and physical modelling of sea-ice dynamics, and drivers of its variability.

Good initial conditions are generally required for a good forecast, however, it is not entirely known to what extent accurate sea-ice initial conditions affect the quality of the forecast and at what timescales.
Exploring seasonal predictions of Arctic sea ice, @guemas2016 found that sea-ice initial conditions are important in autumn to predict summer Arctic sea ice, but the impact wasn’t as dramatic when predicting winter Arctic sea ice.
@day2014 also found seasonally-varying differences in the effect of initialisation, noting that accurate Arctic sea-ice thickness leads to improved sea-ice forecasts initialised in July but not when initialised in January.

For the Antarctic, @holland2013 studied the initial-value predictability of Antarctic sea ice in a perfect model study using the CCSM3 model.
They found that sea-ice and ocean initial conditions provide predictive information to forecast sea-ice edge location several months in advance and that some predictability is retained for up to two years thanks to ocean heat content anomalies that are advected eastward.
Similarly, @morioka2022 studied decadal forecasts of Antarctic sea ice and found that initialising ocean and sea ice improved the correlation between simulated and observed sea-ice concentration evolution in the Amundsen–Bellingshausen Sea.
This is in contrast with @marchi2020, who ran perfect model experiments to argue that uncertainty in the predicted atmospheric state and evolution is the main driver of uncertainty in Antarctic sea-ice extent prediction on seasonal timescales, with sea-ice and ocean initial conditions having lesser importance.
More recently, @xiu2025 contrasted Antarctic sea ice hindcasts based on different data assimilation sources. 
The concluded that hindcast initialised only with atmospheric data lead to good predictions of sea-ice area in many regions that where comparable if not better than hindcasts initialised only with ocean data or with ocean and sea-ice concentration data. 
Furthermore, they find little benefit in assimilation of sea-ice concentration data on top of ocean data. 

It is hard to compare these studies since they are based on forecasts initialised at different times of the year and different frameworks.
@holland2013 ran 20 ensemble members initialised on the 1st of January of a particular year, @marchi2020 ran forecasts from the 1st of March and 1st of September, and @morioka2022 ran forecasts only from the 1st of March.
@xiu2025 was the most comprehensive study, using hindcasts initialised on the 1st of January, April, July and October. 
@marchi2020 also used a coupled ocean–sea-ice model instead of a fully coupled model like @holland2013 and @xiu2025 did.
@morioka2022 and @xiu2025 both compared forecast with observations, but the former used observed sea-ice initial conditions while the latter used initial conditions derived from data asimilationand.
On the other hand, @marchi2020 and @holland2013 were perfect model studies. 

In October 2021 the Australian Bureau of Meteorology (BoM) upgraded the Australian Community Climate and Earth System Simulator – Seasonal (ACCESS-S) from version S1 to S2.
While the base model remained the same, the change in version was focused on using ocean, sea-ice and land initial conditions generated by the BoM instead of depending on the UK Met Office.
Crucially, compared to +S1, +S2 does not assimilate sea-ice observations, so sea ice is only affected by the ocean and atmospheric data assimilation via the coupled integration.

Since model configuration is identical between +S1 and +S2, they form a sort of "opportunistic experiment” where the same forecasting model was run over a long period of time with multiple ensemble forecasts initialised throughout the year, with the only difference being the initial conditions.
This provides an opportunity to test the effect of sea-ice initial conditions on the forecast of sea-ice concentrations and the climate.

In this study we compare sea-ice hindcasts produced by +S1 and +S2.
We focus on seasonality of errors and biases and the effect of the data assimilation system.
This comparison will inform future work with the prediction system as a research tool to better understand the dynamics and variability of the Antarctic sea ice and its impacts on the climate system as well as to explore the potential of using its sea-ice forecasts for decision-making.
The work will also serve as a benchmark for future prediction systems to attempt to improve upon.

# Data and methods

## Forecasting systems {#sec-systems}

```{r}
#| label: ranges
s1_range <- as.Date(c("1990-01-02 UTC", "2013-07-06 UTC"))
s2_range <- as.Date(c("1981-02-02 UTC", "2019-09-06 UTC"))
range_overlap <- as.Date(c("1990-01-02 UTC", "2012-12-01 UTC"))

overlap_range <- function(data, time) {
  time <- deparse(substitute(time))
  data[time %between% range_overlap, env = list(time = time)]
}
```

+S2 [@wedd2022] is the Bureau of Meteorology's seasonal forecast system which became operational in October 2021, replacing the +S1 system [@hudson2017].
The model components of both +S2 and +S1 are identical with the same numbers of levels and resolution.
They consist of the Global Atmosphere 6.0 (GA6) [@williams2015; @waters2017], the Unified Model's Global Land 6.0 [@best2011; @waters2017], NEMO Global Ocean 5.0 [@gurvan2013; @megann2014] and Global Sea Ice 6.0 [CICE; @rae2015].
The atmosphere has a N216 horizontal resolution (\~60 km in the mid-latitudes) with 85 vertical levels.
The land model uses the same horizontal grid as the atmosphere with four soil levels.
The ocean component has a nominal horizontal resolution of 1/4° with 75 vertical levels.
The sea-ice component, based on CICE version 4.1, has the same resolution as the ocean component and five sea-ice thickness categories as well as an open water category.

Both systems take atmospheric initial conditions derived from ERA-interim [@dee2011] for their hindcasts.
The only difference between the hindcasts of the two systems are the ocean and sea-ice initial conditions.

+S1's ocean and sea-ice initial conditions come from the Met Office FOAM system, which uses a multivariate, incremental three-dimensional variational (3D-Var), first-guess-at-appropriate-time (FGAT) data assimilation scheme [@waters2015] and assimilates sea surface temperature (SST), sea surface height (SSH), in situ temperature and salinity profiles, and satellite observations of sea-ice concentration using the EUMETSAT OSISAF product described in the next section.
The way sea-ice thickness is handled is that sea-ice concentration innovations are either added to the first ice category with a fixed thickness of 50cm or removed from the thinnest category first and from thicker categories if needed. 

+S2, on the other hand, is initialised from ocean conditions generated by the BoM weakly coupled ensemble data assimilation scheme described in @wedd2022.
This scheme uses an optimal interpolation method and assimilates temperature and salinity profiles from EN4 [@good2013].
SSTs are nudged to Reynolds OISSTv2.1 [@reynolds2007] in areas where SSTs are over 0°C and Sea Surface Salinity is weakly nudged to the World Ocean Atlas 2013 climatology [@zweng2013].

Of most relevance for this work, sea-ice concentrations are not assimilated in +S2.
Assimilation cycles are performed daily.
The coupled model runs for 24 hours initialised from the previous cycle.
Then the restart file fields of the ocean component are used as first guess in the data assimilation cycle and the innovations are used to build the next ocean initial conditions for the following cycle.
The atmosphere fields from that daily integration are not used and instead the model atmosphere is initialised using ERA-Interim.
The sea-ice initial conditions for the next cycle are the unaltered output of the previous daily integration.
Then the cycle starts again and the coupled model runs for another 24 hours.
During this integration the sea-ice component is affected by the ocean innovations and the new atmosphere initial conditions via the coupler.

The +S1 hindcast set is made up of nine members created by perturbing the atmospheric fields only with a random field perturbation [@hudson2017] and runs for 217 days for the period 1990–2012 initialised at the first of every month.
The +S2 hindcast set used in this study runs for the period 1981–2018.
Ensemble members are created in the same manner as +S1 members, however, due to computing cost limitations, only three members per forecast initialisation date were run for 279 days. 
Bigger ensembles were generated by aggregating several three-member ensembles initialised on successive days [@wedd2022]. 
Here, we build a nine-member time-lagged ensemble from three consecutive three-member forecasts initialised at the first of every month and the two previous days and run for 279 days. 
We analyse the ensemble mean hindcasts unless otherwise specified. 

Although the hindcast periods differ for each forecasting system, we only use data from the overlapping period to compute error measures. 

Anomalies for each hindcast set are taken with respect to their own climatology specific to each initialisation date and forecast lead time, for the period 1990–2012.
This serves as a first-order correction of model bias and drift.
For monthly means, we define "0 lead time months" as the monthly mean forecast of the same month of initialisation. 

Besides sea-ice concentration, we also analyse mean sea-ice thickness, which we compute as total sea-ice volume divided by total sea-ice area. 


## Verification datasets

```{r datasets}
datasets <- list(
  cdr = CDR() |>
    zenodo("NSIDC CDR sea-ice concentration."),
  osi = OSI() |>
    zenodo("OSI sea-ice concentration.")
)
labs_dataset <- c(cdr = "CDR", osi = "OSI")
which_dataset <- "cdr"
```

For verification we use satellite-derived sea-ice concentration, which estimates the proportion of each grid area that is covered with ice.
Datasets derived using different algorithms and satellite platforms, each have their own biases and uncertainties.
Estimates of inter-product uncertainty of sea-ice extent (SIE, defined here as the total region of the Southern Ocean with at least 15% sea-ice cover) are of the order of 0.5 million $km^2$ [@meier2019].
As will be shown below, this spread is minimal compared with the typical errors in the +S2 and +S1 forecasts, so the overall conclusions of this study are independent of the verification dataset used.

We use NOAA/NSIDC's Climate Data Record V4 [CDR; @meier2014] as the primary sea-ice verification dataset.
It takes the maximum value of the NASA Team [@cavalieri1984] and NASA Bootstrap [@comiso2023] sea-ice concentration products to reduce their low concentration bias [@meier2014; @meier2021].
Both source algorithms use data from the Scanning Multichannel Microwave Radiometer (SMMR) on the Nimbus-7 satellite and from the Special Sensor Microwave/Imager (SSM/I) sensors on the Defense Meteorological Satellite Program's (DMSP) -F8, -F11, and -F13 satellites.
The data have a spatial resolution of 25 by 25 km and daily from November 1978 onwards.

The European Organisation for the Exploitation of Meteorological Satellites (EUMETSAT) Ocean and Sea Ice Satellite Application Facility [OSI; @OSISAF] based on the SSMIS sensor is another satellite-derived sea-ice concentration product. 
It is based on mostly the same sensors as the NOAA CDR but computed independently using different algorithms. 
Figures prepared with this dataset are provided in the appendix and do not differ significantly from the ones prepared using CDR.

## Error measures
 
For evaluation purposes, we use a series of measures. 
Sea-ice extent is defined as the area of the ocean at least 15% covered by sea-ice. 
This threshold is motivated by the limitations in satellite retrieval, which is increasingly unreliable for lower sea-ice concentrations [@cavalieri1991].
We evaluate sea-ice extent anomaly hindcasts by their Root Mean Squared Error (RMSE) and correlation coefficient. 
The hindcast ensemble mean sea-ice extent is defined as the mean sea-ice extent of individual ensemble members.

Pan-Antarctic (net) sea-ice extent serves as a hemispheric measure of the amount of sea ice, but it does not take into account the spatial distribution. 
A model could have a relatively accurate extent of the net ice but with different regional distributions. 
To account for location errors, we computed the RMSE of grid-point sea-ice concentration anomalies.

We compute RMSE as the square root of the area-averaged squared differences between grid-point forecasted and observed sea-ice concentration anomalies.
We compute a pan-Antarctic RMSE by averaging over the whole NOAA/NSIDC CDRV4 Southern Hemisphere domain, and also a zonally-varying RMSE computed over 15 longitude slices 24° wide around Antarctica.

All error measures were computed on the NOAA/NSIDC CDRV4 domain grid, to which model output was bilinearly interpolated. 
Note that the ACCESS CICE model grid has resolution between two and three times higher than NOAA/NSIDC CDRV4.

Forecast errors are also compared with hypothetical forecasts based on the persistence of anomalies and on climatology. 
The persistence forecast is generated by extending the observed sea-ice concentration anomalies the day of the forecast initialisation and comparing it with the actual anomalies observed. 
The climatological forecast error is computed as the standard deviation of daily anomalies. 

As a measure of forecast improvement over the hypothetical forecast, we use the skill score [@murphy1985], defined as

$$
S = 1 - \frac{RMSE_{f}}{RMSE_{r}}
$$

Where $RMSE_{f}$ is the RMSE of the forecast, $RMSE_{r}$ is the RMSE of the reference forecast. 
Negative skill score indicates that the forecast is worse than the reference forecast while positive values indicate an improvement. 
A perfect forecast would have zero RMSE and thus a skill score of 1. 

Furthermore, we also compute sea-ice concentration bias, defined as the mean difference between the forecast and observations. 

## Computational procedures 

We performed all analyses in this paper using the R programming language [@rcoreteam2020], using data.table [@dowle2020] and metR [@campitelli2020b] packages.
Significant processing was performed using the CDO command line operators [@schulzweida2023].
All graphics are made using ggplot2 [@wickham2009].
The paper was rendered using knitr and Quarto [@xie2015; @allaire2022].

# Results and discussion

## Bias

```{r extent_daily}
extent_daily <- datasets |>
  lapply(extent) |>
  lapply(\(x) ReadNetCDF(x, "aice")) |>
  rbindlist(idcol = "dataset") |>
  _[, let(lon = NULL, lat = NULL)] |>
  _[, anom := aice - mean(aice, na.rm = TRUE), by = .(yday(time), dataset)] |>
  # some values before 2010 are zero or extremely low. likely due to missing values.
  _[year(time) < 2010 & anom < -2.5e12, aice := NA] |>
  _[, anom := NULL] |>
  _[, time := update(time, hour = 0)]
```

```{r hindcast_extent}
hindcast_extent <- here::here("data/derived/hindcast_extent.csv") |>
  zenodo("Sea-ice extent from hindcast.") |>
  fread() |>
  _[member != "em"] |>
  _[, model := relevel(factor(model), "S2")] |>
  _[, lag := as.numeric(as.Date(time) - as.Date(forecast_time))] |>
  _[lag > 0] |>
  _[!(month(forecast_time) == 1 & member == 7 & model == "S1")] |> # This member is wrong
  _[, .(aice = mean(aice)), by = .(model, measure, forecast_time, time, lag)]
```

```{r extent_climatology}
ranges <- hindcast_extent[, .(range = list(range(time))), by = model]

extent_climatology <- purrr::pmap(ranges, \(model, range) {
  extent_daily |>
    _[time %between% range] |>
    _[aice == 0, aice := NA] |>
    _[!(month(time) == 2 & mday(time) == 29)] |>
    _[, time := update(time, year = 2001)] |>
    _[, average(aice), by = .(time, dataset)] |>
    _[, model := model] |>
    _[]
}) |>
  rbindlist()
```

```{r range_max_lag}
range_max_lag <- hindcast_extent[
  mday(time) == 2,
  .SD[which.max(lag)],
  by = .(forecast_time, model)
] |>
  _[, .(max = max(lag), min = min(lag)), by = model]
```

```{r fig-hindcast-extent}
#| fig-cap: "Row a: Pan-Antarctic daily mean sea-ice extent for all hindcasts initialised on the first of each calendar month for +S1 (column 1; green) and +S2 (column 2; purple). Observed mean sea-ice extent in each corresponding hindcast period is shown in black. Row b: Mean differences between the forecast and the observed values. Circles represent the initial conditions at the start of forecasts (i.e., the first of every month), and triangles represent the mean values forecasted for the first of every month the lead time corresponding to the maximum lead time in S1 (between 213 and 216 days, depending on the month)."
#| fig-height: 4

hindcast_extent[, .(model, forecast_time, time, Forecast = aice)] |>
  overlap_range(forecast_time) |>
  extent_daily[i = _, on = c("time"), allow.cartesian = TRUE] |>
  _[, Difference := Forecast - aice] |>
  _[, aice := NULL] |>
  tidyfast::dt_pivot_longer(cols = c(Forecast, Difference)) |>
  _[,
    average(value),
    by = .(
      dataset,
      model,
      measure = name,
      forecast_time = update(forecast_time, year = 2001),
      lag = as.numeric(as.Date(time) - forecast_time)
    )
  ] |>
  _[, time := forecast_time + lag] |>
  _[, time2 := update(time, year = 2001)] |>
  _[, month := lubridate::month(forecast_time, label = TRUE)] |>
  _[, model := factor(model, levels = c("S1", "S2"))] |>
  _[, measure := factor(measure, levels = c("Forecast", "Difference"))] |>
  _[dataset == which_dataset] |>
  ggplot(aes(time2, estimate)) +
  geom_line(
    data = extent_climatology[dataset == which_dataset] |>
      _[, measure := factor("Forecast", levels = c("Forecast", "Difference"))],
    aes(x = as.Date(time), colour = dataset, group = dataset)
  ) +

  geom_hline(
    data = data.table(
      y = 0,
      measure = factor("Difference", levels = c("Forecast", "Difference"))
    ),
    aes(yintercept = 0)
  ) +
  geom_line(aes(
    group = interaction(forecast_time, time != time2),
    colour = model
  )) +
  # geom_point(
  #   data = \(x) {
  #     x[
  #       mday(time) == 2,
  #       .SD[which.max(lag)],
  #       by = .(forecast_time, model, measure)
  #     ]
  #   },
  #   aes(fill = model),
  #   shape = 24,
  #   size = 2.2
  # ) +

  geom_point(
    data = \(x) {
      x[
        mday(time) == 2,
        .SD[lag == max(lag[model == "S1"])],
        by = .(forecast_time, measure)
      ]
    },
    aes(fill = model),
    shape = 24,
    size = 2.2
  ) +

  geom_point(data = \(x) x[lag == 1], aes(fill = model), shape = 21, size = 2) +

  scale_x_date(
    NULL,
    date_breaks = "2 month",
    date_labels = "%b",
    expand = c(0, 0)
  ) +
  scale_y_continuous(NULL, labels = labels_extent) +
  scale_color_models() +
  scale_fill_models() +
  guides(fill = "none") +
  # scale_color_manual(values = cetcolor::cet_pal(12, name = "c2"),
  #                    aesthetics = c("fill", "color"), guide = "none") +
  facet_grid(
    measure ~ model,
    scales = "free_y",
    labeller = labeller(model = labels_models)
  ) +
  tag_facets(tag = "rc") +
  coord_cartesian(clip = "off")
```


@fig-hindcast-extent (@fig-hindcast-extent-osi for OSI) shows mean sea-ice extent of the +S1 and +S2 hindcasts (row a) and their differences from mean sea-ice extent of +CDR (row b).
Mean extent at the first of every month is indicated with circles for the initial conditions and with triangles for the longest lead time possible for each model (between `r range_max_lag[model == "S2", min]` and `r range_max_lag[model == "S2", max]` days for +S2 and between `r range_max_lag[model == "S1", min]` and `r range_max_lag[model == "S1", max]` days for +S1).
At this long lead time, information of the initial conditions is essentially lost and the forecast reverts close to each model's preferred equilibrium state.

+S2 initial conditions (circles in [Fig. @fig-hindcast-extent] column 2) show an overall negative bias, especially in the late summer-early autumn, while +S1 initial conditions (circles in [Fig. @fig-hindcast-extent] column 1) are very close to observations, as expected from the assimilation of sea-ice observations to produce the initial conditions of +S1.
Both systems' equilibrium states (triangles) show negative biases of sea-ice extent, particularly in the growth phase of late-autumn and winter months.
This is due primarily to the melt season being longer than in observations and with faster melt between January and March and the growing seasons being shorter with slower growth during March and April.
This is then followed by faster growth between May and July (@fig-mean-growth and @fig-mean-growth-osi).
Many sea-ice models exhibit this systematic underestimation during the sea-ice minimum and early freezing season [@bushuk2021; @massonnet2023], which could indicate problems in the representation of thermodynamics in the model [@zampieri2019].
It is also not surprising that both forecasting systems converge to a similar equilibrium state because they share the same model formulation.

The difference between the initial conditions (circles) and the model equilibrium state (triangles) can be mostly attributed to the effect of data assimilation, which in +S2 is due solely to the coupling of sea-ice with the atmosphere and the ocean. 
From April to September, in +S2 circles are closer to observations than the triangles are, indicating that the information from the ocean and atmosphere data assimilation is affecting sea ice and improving the initial conditions.
During these months, +S1 can overestimate the sea-ice extent at short lead time.
For the rest of the year circles are overlaid with triangles in +S2, indicating that the ocean and atmosphere data assimilation is not affecting sea ice and that this component of the model is virtually free-running.

```{r extent-delta-compute}
n <- 366 * 3
w <- 11

dif_hindcast <- hindcast_extent |>
  overlap_range(forecast_time) |>
  copy() |>
  _[order(time)] |>
  _[,
    dif := c(NA, diff(aice) / diff(as.numeric(as.Date(time)))),
    by = .(model, forecast_time)
  ] |>
  _[,
    dif := frollmean(dif, n = w, align = "center"),
    by = .(model, forecast_time)
  ] |>
  na.omit() |>
  _[,
    average(dif),
    by = .(model, month(forecast_time), time = update(time, year = 2000))
  ]

dif_extent <- extent_daily |>
  overlap_range(time) |>
  _[order(time)] |>
  _[,
    dif := c(NA, diff(aice) / diff(as.numeric(as.Date(time)))),
    by = .(dataset)
  ] |>
  na.omit() |>
  _[, dif := frollmean(dif, n = w, align = "center"), by = .(dataset)] |>
  _[, average(dif), by = .(dataset, time = update(time, year = 2000))]
```

```{r fig-mean-growth}
#| fig-cap:  Mean daily sea-ice extent growth ($10^6 km^2/day$) in +S1 (green) and +S2 (purple) hindcasts and observations (black), computed as the mean daily differences in sea-ice extent between each date and the next for each forecast month. Values are smoothed with a 11-day running mean.
#| fig-height: 4

dif_extent |>
  _[dataset == which_dataset] |>
  ggplot(aes(time, estimate)) +
  geom_hline(yintercept = 0, color = "gray50", linewidth = 0.2) +
  geom_line(aes(color = dataset)) +
  geom_line(
    data = dif_hindcast,
    aes(color = model, group = interaction(model, month, month > month(time)))
  ) +
  scale_x_datetime(
    NULL,
    date_breaks = "1 month",
    date_labels = "%b",
    expand = c(0, 0)
  ) +
  scale_y_continuous(NULL, labels = \(x) {
    labels_extent(x, units = "M km²/day")
  }) +
  scale_color_models() +
  coord_cartesian(xlim = as.POSIXct(c("2000-01-01", "2000-12-31"), tz = "UTC"))
```


```{r hindcast_clim}

hindcast_clim <- hindcast_0lag_clim() |>
  _[, ReadNetCDF(file, "aice"), by = .(model)] |>
  _[, month := month(time)] |>
  _[, time := NULL]

monnb <- function(d) {
  lt <- as.POSIXlt(as.Date(d, origin = "1900-01-01"))
  lt$year * 12 + lt$mon
}
mondf <- function(d1, d2) {
  monnb(d2) - monnb(d1)
}
obs_clim <- lapply(datasets, \(x) {
  x |>
    climatology() |>
    cdo_ymonmean() |>
    cdo_execute() |>
    ReadNetCDF(c(obs = "aice"))
}) |>
  rbindlist(idcol = "dataset") |>
  _[, month := month(time)]

hindcast_clim <- hindcast_clim |>
  merge(obs_clim)
```



```{r}
#| label: plot_bias

circle <- function(center, radius, res = 80) {
  t <- seq(0, 2 * pi, length.out = res)
  data.frame(x = cos(t) * radius, y = sin(t) * radius)
}


box <- c(xmin = -3937500, xmax = 3937500, ymin = -3937500, ymax = 4337500)
geom_coords <- function() {
  lats <- seq(-90, -40, by = 10)[-1]

  ys <- proj4::project(
    list(lon = rep(0, length(lats)), lat = lats),
    proj = sic_projection
  )$y

  latitudes <- lapply(setNames(ys, lats), \(x) circle(0, x)) |>
    data.table::rbindlist(idcol = "lat")

  lons <- seq(0, 360, length.out = 9)[-1]
  longitudes <- data.table::CJ(lon = lons, lat = c(lats[1], max(lats))) |>
    _[, c("x", "y") := proj4::project(list(lon, lat), proj = sic_projection)] |>
    _[]

  list(
    geom_path(
      data = latitudes,
      aes(x, y, group = lat),
      inherit.aes = FALSE,
      linewidth = 0.1,
      colour = "gray50",
      alpha = 0.5
    ),
    geom_path(
      data = longitudes,
      aes(x, y, group = lon),
      inherit.aes = FALSE,
      linewidth = 0.1,
      colour = "gray50",
      alpha = 0.5
    ),
    geom_label(
      data = data.frame(x = 0, y = ys, label = lats),
      aes(x = x, y = y, label = label),
      size = 1,
      label.size = 0,
      inherit.aes = FALSE
    )
  )
}

plot_bias <- lapply(c("S2", "S1"), \(x) {
  hindcast_clim |>
    _[model == x] |>
    _[dataset == which_dataset] |>
    # dcast(time + xgrid + ygrid ~ model, value.var = "aice") |>
    # setnames(which_dataset, "obs") |>
    ggplot(aes(xgrid, ygrid)) +
    geom_contour_fill(
      aes(z = aice - obs, fill = after_stat(level)),
      breaks = AnchorBreaks(binwidth = 0.1, exclude = 0)(c(-0.999, 0.999))
    ) +
    scale_fill_divergent_discretised(
      paste0("Bias with respect to ", labels_models[which_dataset]),
      low = pink,
      high = blue
    ) +
    # geom_polygon(data = ice_shelves_nsicd, aes(X, Y, group = group),
    #              colour = "black", fill = "#FAFAFA") +
    geomcoord_antarctica +
    geom_antarctica_fill +
    facet_wrap(~month, labeller = labeller(month = labels_month)) +
    wide_legend +
    tag_facets() +
    geom_coords() +
    ggplot2::coord_sf(
      xlim = box[1:2],
      ylim = box[3:4],
      crs = sic_projection,
      lims_method = "box",
      label_axes = "----"
    )
})
```

```{r fig-bias-1}
#| fig-cap: Ensemble mean difference between monthly sea-ice concentration of +S2 ensemble mean forecast at 0-month lead time (monthly mean values forecasted from the forecast initialised at the first of the month) and observations ({labs_dataset[which_dataset]}).
#| fig-height: 6

plot_bias[[1]]
```


```{r fig-bias-2}
#| fig-cap: Same as @fig-bias-1 but for +S1.
#| fig-height: 6
plot_bias[[2]]
```


To further understand the bias in +S2, @fig-bias-1 (@fig-bias-1-osi) shows spatial patterns of the differences of monthly mean sea-ice concentrations between NOAA/NSIDC CDRV4 and +S2 hindcasts at the shortest monthly lead time.
From October to May, the model underestimates sea-ice concentrations in most regions except for the inner Weddell Sea in April and May, where sea-ice concentrations saturate to 1 both in the observations and forecasts.
In winter, the differences are limited to a narrow band around the sea-ice edge with slight positive biases in the African sector of East Antarctica and negative biases around the Indian Ocean sector which partially compensate, resulting in the near-zero extent bias seen in those months (@fig-hindcast-extent).

+S1 has a comparatively smaller overall bias (@fig-bias-2 and @fig-bias-2-osi). 
The largest values are found between April and June, when the faster growth results in large positive bias along the sea-ice edge, and in January, when the faster melt leads to large negative bias in the Weddell and Amundsen Seas. 

\FloatBarrier

## Anomaly errors

```{r fig-extent-anom}
#| fig-cap: Monthly mean sea-ice extent anomalies of the observations (black) and forecasts from +S1 (right column; purple) and +S2 (left column; green) at lead times of 0, 2, 4, and 6 months. The RMSE and correlation with their respective 95% confidence interval during the overlapping period of +S1 and +S2 hindcasts (1990–2012) are shown on the top left and bottom left of each panel respectively
#| fig-height: 6

months_difference <- function(x, y) {
  lubridate::interval(y, x) %/% months(1)
}

monthly_extent <- extent_daily |>
  _[dataset == which_dataset] |>
  _[aice == 0, aice := NA] |>
  _[,
    .(aice = mean(aice)),
    by = .(dataset, time = as.Date(round_date(time, "month")))
  ] |>
  _[,
    aice := Anomaly(aice, year(time) %between% c(1990, 2012), na.rm = TRUE),
    by = .(month(time), dataset)
  ] |>
  _[]

N <- uniqueN(monthly_extent$time)

hindast_extent_monthly <- hindcast_extent |>
  copy() |>
  _[, .(aice = mean(aice)), by = .(model, measure, forecast_time, time, lag)] |>
  _[,
    aice := aice - mean(aice[year(time) %between% c(1990, 2012)]),
    by = .(lag, model, time = update(time, year = 2000))
  ] |>
  _[,
    .(forecast = mean(aice)),
    by = .(model, forecast_time, time = as.Date(round_date(time, "month")))
  ] |>
  _[, lag := months_difference(time, forecast_time)]

hindast_extent_monthly |>
  _[, merge(.SD, monthly_extent, all = TRUE), by = .(model, lag)] |>
  _[lag %in% (c(1, 3, 5, 7) - 1)] |>
  _[time %between% s2_range] |>
  _[, model := factor(model, levels = c("S1", "S2", which_dataset))] |>
  ggplot(aes(time)) +
  geom_line(aes(y = aice, group = dataset, color = dataset), linewidth = 0.2) +
  geom_line(aes(y = forecast, color = model), linewidth = 0.2) +
  geom_text(
    data = \(x) {
      x[
        time %between% range_overlap,
        rmse(aice, forecast, signif = 2, scale = 1e12),
        by = .(lag, model)
      ]
    },
    aes(label = paste0("RMSE: ", text)),
    x = -Inf,
    y = Inf,
    size = 2.5,
    vjust = 2.5,
    hjust = 0
  ) +

  geom_text(
    data = \(x) {
      x[
        time %between% range_overlap,
        correlate(aice, forecast),
        by = .(lag, model)
      ]
    },
    aes(label = paste0("cor: ", text)),
    vjust = -1.5,
    hjust = 0,
    size = 2.5,
    x = -Inf,
    y = -Inf
  ) +

  scale_y_continuous(NULL, labels = labels_extent) +
  scale_x_date(NULL, expand = c(0, 0)) +
  facet_grid(
    lag ~ model,
    labeller = labeller(lag = \(x) paste0("Month: ", x), model = labels_models)
  ) +
  scale_color_models() +
  scale_fill_models()
```

@fig-extent-anom (@fig-extent-anom-osi) shows monthly sea-ice extent anomalies forecasted at selected lead times.
Compared with +S1, +S2 anomaly forecasts are relatively poor (large RMSE) even for the first month (lead time 0), whereas +S1 forecasts have lower RMSE than +S2 even at a lead time of four months.
+S2 shows much larger interannual variability than observations, with dramatic lows between 1995 and 2007, and highs between 2007 and 2015.

Unexpectedly, for +S2, RMSE improves with lead time, even though the correlation degrades with lead time.
This effect is seen in all months except from July to September [@fig-extent-rmse-month].
This is puzzling behaviour that goes contrary to what is usually seen in prediction models.
The explanation seems to be the mentioned increased interannual variability.
@fig-extent-sd (@fig-extent-sd-osi) shows the interannual standard deviation of monthly sea-ice extent of the forecasts as a function of lead time compared with observations.
+S1 standard deviation lies within the observed standard deviation regardless of lead time, while +S2 standard deviation is more than twice that of observations at zero lead time and only approaches the observed value at nine months lead time for most months.

```{r fig-extent-rmse-month_build}

fig_extent_rmse_month <- hindast_extent_monthly |>
  _[, merge(.SD, monthly_extent, all = TRUE), by = .(model, lag)] |>
  _[
    time %between% range_overlap,
    rmse(aice, forecast, signif = 2, scale = 1e12),
    by = .(lag, model, month(time))
  ] |>
  # _[, FitLm(estimate, lag), by = .(model, month)] |>
  # _[term == "lag"] |>
  # _[model == "S2"] |>
  # _[, estimate := estimate / estimate[1], by = .(month, model)] |>
  # _[, up := FitLm(estimate, lag)$estimate[2] > 0, by = .(month)] |>
  ggplot(aes(lag, estimate)) +
  geom_ribbon(aes(ymin = low, ymax = high, fill = model), alpha = 0.5) +
  geom_line(aes(color = model)) +
  scale_y_continuous("Sea-ice extent anomaly RMSE.") +
  scale_x_continuous(
    "Lead time [months]",
    breaks = c(0:9),
    expand = c(0, 0),
    trans = scales::reverse_trans()
  ) +
  scale_color_models() +
  scale_fill_models() +
  facet_wrap(~month, labeller = labeller(month = labels_month))
```


```{r fig-extent-sd}
#| fig-cap: Interannual standard deviation with 95% confidence interval of monthly mean sea-ice extent forecasted for each month. We standardise the standard deviation by that month’s sea-ice extent observation standard deviation. +S1 and +S2 at different lead times. Each panel indicates the target month. Note the reverse horizontal axis.
#| fig-height: 5

obs_sd <- monthly_extent[
  time %between% range_overlap,
  sd_ci(aice, na.rm = TRUE),
  by = .(month(time), dataset)
] |>
  _[, .(dataset, month, obs_sd = estimate)]

hindcast_extent_sd <- hindast_extent_monthly[
  time %between% range_overlap,
  sd_ci(forecast, na.rm = TRUE),
  by = .(model, lag, month(time))
]

hindcast_extent_sd |>
  _[obs_sd, on = .NATURAL] |>
  _[dataset == which_dataset] |>
  ggplot(aes(lag, estimate / obs_sd)) +
  # geom_rect(data = obs_sd, inherit.aes = FALSE,
  #            aes(ymin = low, ymax = high,  fill = dataset, xmin = -Inf, xmax = Inf),
  #               alpha = 0.5) +
  # geom_hline(data = obs_sd, aes(yintercept = estimate, color = dataset)) +
  geom_ribbon(
    aes(ymin = low / obs_sd, ymax = high / obs_sd, fill = model),
    alpha = 0.5
  ) +
  geom_line(aes(color = model)) +

  scale_y_continuous("Standardised sea-ice extent standard deviation") +
  scale_x_continuous(
    "Lead time [months]",
    breaks = c(0:9),
    expand = c(0, 0),
    trans = scales::reverse_trans()
  ) +
  scale_color_models() +
  scale_fill_models() +
  facet_wrap(~month, labeller = labeller(month = labels_month))
```

```{r plot_forecast_example}

plot_forecast_example <- function(sample_date) {
  mlag <- 0
  forecast_date <- as.Date(sample_date)
  month(forecast_date) <- month(sample_date) - mlag
  mday(forecast_date) <- 1

  aice_forecast_example <- c(
    lapply(c("S1", "S2"), \(model) {
      month_clim <- formatC(month(forecast_date), width = 2, flag = "0")
      clim <- here::here(glue::glue(
        "data/derived/climatology/{model}/{month_clim}/em.nc"
      )) |>
        zenodo("{model} sea-ice concentration climatology for {month_clim}")

      hindcast(forecast_date, model, 1) |>
        zenodo("{model} hindcast for initialised on {forecast_date}") |>
        cdo_ydaysub(clim) |>
        cdo_execute() |>
        ReadNetCDF("aice", subset = list(time = sample_date)) |>
        _[, model := model]
    }),
    list(
      CDR() |>
        anomalies() |>
        ReadNetCDF("aice", subset = list(time = sample_date)) |>
        _[, model := "cdr"]
    )
  ) |>
    rbindlist()

  thickness_example <- c(
    S2 = hindcast(forecast_date, "S2", members = 1, variable = "hi") |>
      zenodo(
        "S2 sea-ice thickness hindcast for {forecast_date} (1st ensemble member)"
      ),
    S1 = hindcast(forecast_date, "S1", members = 1, variable = "hi") |>
      zenodo(
        "S1 sea-ice thickness hindcast for {forecast_date} (1st ensemble member)"
      )
  ) |>
    purrr::imap(\(f, model) {
      f |>
        glue::glue() |>
        cdo_selname("hi") |>
        remap_cdr() |>
        cdo_execute(cache = TRUE) |>
        ReadNetCDF("hi", subset = list(time = sample_date))
    }) |>
    rbindlist(idcol = "model")

  b <- seq(-0.5, 0.5, by = 0.1)
  b <- b[b != 0]

  aice_forecast_example |>
    _[, model := factor(model, levels = c("S1", "S2", which_dataset))] |>
    ggplot(aes(xgrid, ygrid)) +
    geom_contour_fill(
      aes(z = aice, fill = ..level..),
      breaks = AnchorBreaks(binwidth = 0.1, exclude = 0)
    ) +
    scale_fill_divergent_discretised(
      "Sea-ice concentration anomaly",
      low = pink,
      high = blue
    ) +
    geom_antarctica_fill +
    geomcoord_antarctica +
    facet_wrap(~model, labeller = labeller(model = labels_models)) +
    wide_legend +

    thickness_example |>
      _[, model := factor(model, levels = c("S1", "S2", "Difference"))] |>
      ggplot(aes(xgrid, ygrid)) +
    geom_contour_fill(
      aes(z = hi, fill = ..level..),
      breaks = c(AnchorBreaks(exclude = 0, binwidth = 0.25)(c(0, 2)), Inf)
    ) +
    # as.discretised_scale(scale_fill_viridis_c)("Sea ice thickness",
    #                                            guide = guide_colorsteps(order = 9)) +
    scale_fill_divergent_discretised(
      "Sea-ice thickness",
      high = scales::muted("#5ab4ac"),
      guide = guide_colorsteps(order = 9)
    ) +
    ggnewscale::new_scale_fill() +

    geom_contour_fill(
      data = \(x) {
        x |>
          dcast(time + xgrid + ygrid ~ model, value.var = "hi") |>
          _[,
            model := factor("Difference", levels = c("S1", "S2", "Difference"))
          ]
      },
      aes(z = S1 - S2, fill = ..level..),
      breaks = c(-Inf, b, Inf)
    ) +

    scale_fill_divergent_discretised(
      "Sea-ice thickness difference",
      guide = guide_colorsteps(order = 0)
    ) +
    geom_antarctica_fill +
    geomcoord_antarctica +
    wide_legend +
    facet_wrap(~model, labeller = labeller(model = labels_models)) +
    plot_layout(ncol = 1) &
    geom_coords() &
    ggplot2::coord_sf(
      xlim = box[1:2],
      ylim = box[3:4],
      crs = sic_projection,
      lims_method = "box",
      label_axes = "----"
    )
}

```

+S2 forecasts of sea-ice extent anomalies seem to align moderately well with observations (leading to moderately high correlation) but their magnitude is overestimated (leading to large errors).
This could be caused by +S2 sea ice being much more sensitive to atmospheric and oceanic forcing, perhaps due to lower thickness. 

As an example, @fig-forecast-example shows sea-ice concentration anomalies (top row) and sea-ice thickness and the difference between the two models (bottom row) for `r format(as.Date("2008-05-02"), "%e %B %Y")` initialised one day prior; being that close to initialisation date, these are very approximately the initial conditions. 
+S1 sea-ice concentrations anomalies are very close to observations as expected from the system assimilating these data. 
+S2 sea-ice concentration anomalies, are not as close, but the large-scale pattern is aligned with observations.
The system simulates large positive anomalies in the Weddell and Ross Seas and slight negative anomalies in the Amundsen and Bellingshausen Seas.
The fact that +S2 can simulate this pattern without assimilating sea-ice data suggests that atmospheric and oceanic forcing were the dominant drivers.
However, the magnitude of the sea-ice concentration anomalies is too big. 

```{r fig-forecast-example}
#| fig.cap: +S1 and +S2 hindcasts for 2 May 2008 at one day lead time. Top row shows sea-ice concentration anomalies forecasted by each system and the observations. Bottom row shows forecasted sea-ice thickness and the difference between +S1 and +S2.
#| fig-height: 6

plot_forecast_example(as.Date("2008-05-02"))
```

```{r hindcast_mean_thickness}
hindcast_mean_thickness_data <- hindcast_mean_thickness() |>
  _[,
    ReadNetCDF(mean_thickness, c(thickness = "hi")),
    by = .(model, forecast_time)
  ] |>
  _[, time := as.Date(update(time, day = 1))] |>
  _[, lag := months_difference(time, forecast_time)] |>
  _[lag > 0]
```

```{r}
#| label: fig-mean-thickness
#| fig-cap: Mean and 95% interval of monthly mean sea-ice thickness for +S1 and +S2 at different lead times. Each panel indicates the target month. Note the reverse horizontal axis.
#| fig-height: 5

hindcast_mean_thickness_data[
  time %between% range_overlap,
  average(thickness),
  keyby = .(lag, model, month(time))
] |>
  ggplot(aes(lag, estimate)) +
  geom_ribbon(aes(ymin = low, ymax = high, fill = model), alpha = 0.5) +
  geom_line(aes(color = model)) +
  # geom_smooth(method = "lm", aes(color = model)) +
  scale_color_models() +
  scale_fill_models() +
  scale_y_continuous("Mean sea-ice thickness") +
  scale_x_continuous(
    "Lead time [months]",
    breaks = c(0:9),
    expand = c(0, 0),
    trans = scales::reverse_trans()
  ) +
  facet_wrap(~month, labeller = labeller(month = labels_month))
```

The reason for the increased variability in +S2 is not clear. 
One plausible explanation is that +S2 simulates thinner sea ice than +S1 (@fig-forecast-example bottom row) that is more sensitive to atmospheric and oceanic forcing (@fig-hi-var-cor).
@fig-mean-thickness, shows that indeed +S2 simulates thinner sea ice compared to +S1 overall at almost all lead times and in all months except for summer at short lead times (Dec-Jan, 0-1 months; Feb-Mar, 0-2 months).
However, @fig-mean-thickness also shows that sea-ice thickness tends to decrease at longer leadtimes, when +S2' variability converges to observed variability. 
Furthermore, the thickness difference between +S1 and +S2 is large also in the months in which +S2 sea-ice extent variabilty is comparable to +S1 and +S2 RMSE doesn't decrease with lead time, such as August and September. 

Even if +S2's sea ice is is more sensitive to forcing than +S1 by being overall thinner, this is not a sufficient explanation for the pattern of increased variability at shorter lead times and absent in particular months.
Instead, the streangth of the atmospheric and oceanic forcing might also be playing a role. 
The fact that +S1 and +S2 share the same model configuration and that the increased variability is more extreme at short lead times ([Fig. @fig-extent-sd]) suggests that the data assimilation procedure might also be partly responsible. 
We speculate that it is possible that sea-ice in the +S2 system is left in an unbalanced state after assimilating atmospheric and oceanic data but not sea-ice data, leading to large responses that are amplified by the thinner ice. 
Teasing this out would require an in-depth analysis of the data assimilation innovations which is out of scope for this work and requires data that is not available.

```{r errors}
errors <- here::here("data/derived/rmse.Rds") |>
  zenodo("Hindcast RMSE of sea-ice concentration anomalies") |>
  readRDS() |>
  overlap_range(time_forecast) |>
  _[value < 0.01, value := NA] |>
  _[value > 3e13, value := NA] |>
  _[, time := as.Date(time)] |>
  _[, lag := as.numeric(time - time_forecast)] |>
  _[version == "persistence", member := "0em"] |>
  _[member == "0em"] |>
  _[lag > 0] |>
  _[!(month(time_forecast) == 1 & member == "07" & version == "S1")] # This member is wrong
```

```{r clim_std}
clim_std <- datasets |>
  lapply(\(x) {
    x |>
      anomalies(climatology(x)) |>
      cdo_sqr() |>
      cdo_fldmean() |>
      cdo_sqrt() |>
      cdo_execute() |>
      ReadNetCDF(c(climatology = "aice")) |>
      _[, let(lat = NULL, lon = NULL)]
  }) |>
  rbindlist(idcol = "obs_dataset") |>
  _[, time := as.Date(time)] |>
  _[climatology == 0, climatology := NA] |>
  _[climatology < 0.05, climatology := NA]
```

```{r clim_std2}
clim_std2 <- clim_std[errors, on = .NATURAL] |>
  _[, let(climatology = NULL, value = climatology)] |>
  na.omit() |>
  _[, version := "climatology"]
```

```{r t_test}
t_test <- function(x, y, ...) {
  test <- t.test(x, y, ...)

  list(
    low = test[["conf.int"]][1],
    high = test[["conf.int"]][2],
    estimate = -diff(test$estimate),
    p.value = test[["p.value"]]
  )
}

max_lag <- errors[version == "S1", max(lag)]

dif <- errors |>
  na.omit() |>
  # _[obs_dataset == which_dataset] |>
  _[measure == "rmse"] |>
  _[version != "persistence"] |>
  _[lag <= max_lag] |>
  dcast(
    obs_dataset + time + time_forecast + member + lag ~ version,
    value.var = "value"
  ) |>
  _[,
    t_test(na.omit(S1), na.omit(S2)),
    by = .(obs_dataset, lag, month(time_forecast))
  ] |>
  _[p.value > 0.01] |>
  _[, .SD[which.min(lag)], by = .(obs_dataset, month)] |>
  _[order(month), .(month, lag, obs_dataset)]

```

```{r fig-rmse}
#| fig-cap: Mean RMSE of sea-ice concentration anomalies as a function of forecast lead time for all forecasts initialised on the first of each month compared with a reference forecast of persistence of anomalies (black) and climatology (gray). The shading indicates the 95% confidence interval of the mean. Only the first 150 days are shown. In parentheses, the shortest time at which +S1 and +S2 mean RMSE is not statistically different at the 99% confidence level. Note that the horizontal axis uses a square root transformation to expand the shorter lead times.
#| fig-height: 5

month_from_lag <- function(day = 1, step = 2) {
  force(day)
  force(step)
  function(x) {
    unique(x[, .(lag, month)])[,
      time := make_date(month = month, day = 1) + lag
    ] |>
      _[
        mday(time) == day,
        .(lag, month, month2 = lubridate::month(time, label = TRUE))
      ] |>
      _[, .SD[((seq_len(.N) + 1) %% step) == 0], by = month]
  }
}

labels_month_significant <- setNames(
  paste0(month.abb, " (", dif[obs_dataset == which_dataset]$lag, ")"),
  1:12
)

errors2 <- clim_std2 |>
  rbind(errors) |>
  _[measure == "rmse"] |>
  _[, time2 := update(time, year = 2000)] |>
  # merge(clim_std) |>
  _[,
    average(value),
    by = .(lag, version, measure, month(time_forecast), obs_dataset)
  ]

errors2 |>
  _[obs_dataset == which_dataset] |>
  _[lag <= 150] |>
  ggplot(aes(lag, estimate)) +

  geom_vline(
    data = NULL,
    xintercept = c(5, 15),
    colour = "gray50",
    linewidth = 0.1
  ) +

  geom_text(
    data = data.frame(x = c(5, 15), y = -Inf),
    aes(x, y, label = x),
    size = 2.5,
    vjust = 0
  ) +

  geom_vline(
    data = month_from_lag(1, 1),
    aes(xintercept = lag),
    colour = "gray50",
    linewidth = 0.1
  ) +

  geom_text(
    data = month_from_lag(15, 1),
    aes(y = Inf, label = month2),
    size = 2.5,
    vjust = 1
  ) +

  geom_text(
    data = month_from_lag(1, 1),
    aes(y = -Inf, label = lag),
    size = 2.5,
    vjust = 0
  ) +
  ggblend::blend(
    blend = "multiply",
    list(
      geom_ribbon(
        # data = \(x) x[version %in% c("S1", "S2")],
        aes(
          ymin = low,
          ymax = high,
          # color = version,
          fill = version,
          group = interaction(obs_dataset, version)
        ),
        alpha = 0.3
      ),
      geom_line(aes(color = version, group = interaction(obs_dataset, version)))
    )
  ) +
  # geom_line(aes(y = clim_std)) +
  scale_x_continuous(
    breaks = NULL,
    expand = c(0, 0),
    transform = scales::transform_sqrt()
  ) +
  labs(y = "RMSE", x = "Lead time [days]") +
  scale_color_models() +
  scale_fill_models(guide = "none") +
  facet_wrap(
    ~month,
    labeller = labeller(month = labels_month_significant),
    ncol = 3
  ) +
  coord_cartesian(clip = "off")
```

To assess +S2 forecasts in more detail, we compute error measures for all hindcasts started on the 1st of every month.
@fig-rmse (@fig-rmse-osi) shows the mean RMSE of sea-ice concentration anomalies for +S1 and +S2 hindcasts compared against persistence and climatological forecasts used as a benchmark.
Due to errors in the initial conditions, it is expected that persistence forecasts would be better than the model forecasts at very short lead times, but that the persistence forecast errors would grow faster and may eventually surpass the model forecast errors.
The black line shows that the persistence forecast error indeed grows rapidly and reaches its maximum in about 30 days for most months except for February, when it grows much slower.
The +S1 forecast errors grow slower than persistence forecast errors and remain lower after less than 10 days on average.
The +S2 forecast error starts high in all months and is lower than the persistence forecast error after more than 15 days in most months except for forecast initialised in February, when it takes 80 days.

At longer lead times, it is more appropriate to compare errors with the climatological forecast error. 
The lead time at which +S1 forecast error is higher than the climatological forecast error varies between more than 60 and less than 20 days depending on forecast initialisation month with the minimum in June.
+S2 forecasts never have lower error than climatology, on the other hand, except marginally in October forecasts. 

In summary, +S1 forecasts have a wider lead time window in the summer than the other seasons and is not better than both benchmarks at forecasting June sea-ice concentration anomalies. 
Forecasts initialised between May and July are particularly poor, and June cannot be forecasted better than the benchmarks suggesting a winter barrier in predictive skill (
@fig-lead-time-window and @fig-lead-time-window-osi). 
This is similar to the mid-winter loss of predictability observed by @libera2022, who attributed it to deep warm water entraining into the mixed layer.


```{r}
#| label: fig-improvement
#| fig-cap: Shortest lead time at which +S1 and +S2 mean RMSE is not statistically different at the 99% confidence level.
#| fig-height: 2

median_improvement <- dif |>
  _[obs_dataset == which_dataset] |>
  _[, median(lag)]

dif |>
  _[obs_dataset == which_dataset] |>
  ggplot(aes(month, lag)) +
  geom_col(fill = "gray20") +
  # geom_line() +
  # geom_point() +
  scale_y_continuous(
    "Lead time [days]",
    limits = c(0, NA),
    breaks = scales::pretty_breaks(8)
  ) +
  scale_x_continuous(
    "Month",
    labels = month.abb,
    breaks = 1:12,
    expand = c(0, 0)
  )
```

We quantify the improvement in sea-ice forecast between +S2 and +S1 in @fig-improvement by computing the shortest lead time in which the mean RMSE of each system's forecast are not statistically different. 
The median improvement is around `r floor(median_improvement)` days but the biggest improvement is between February and April, where +S1 forecasts have lower RMSE than +S2's for more between 65 and 125 days. 
The smallest difference, on the other hand, is in winter, when +S1's forecast are statistically indistinguishable from +S2's after less than 20 days. 
Assimilation of sea-ice concentration data seems to have a much greater effect in the late summer than in the winter.

```{r topo_lonlat}
topo_lonlat <- here::here("data/raw/ETOPO.nc") |>
  zenodo("ETOPO topography") |>
  ReadNetCDF() |>
  na.omit()
```

```{r clim_std_lon} 
clim_std_lon <- datasets |>
  lapply(\(x) {
    x |>
      anomalies(climatology(x)) |>
      cdo_sqr() |>
      cdo_remapmean("r15x360") |>
      cdo_mermean() |>
      cdo_sqrt() |>
      cdo_execute() |>
      ReadNetCDF(c(climatology = "aice")) |>
      _[, let(lat = NULL, lon = lon + 12)]
  }) |>
  rbindlist(idcol = "obs_dataset") |>
  _[, time := as.Date(time)] |>
  _[climatology == 0, climatology := NA] |>
  _[climatology < 0.05, climatology := NA]

```

```{r rmse_lon_mean}
rmse_lon_mean <- here::here("data/derived/rmse_lon.Rds") |>
  zenodo("Hindcast RMSE in latitude slices") |>
  readRDS() |>
  overlap_range(time_forecast) |>
  _[version == "persistence", member := "0em"] |>
  _[member == "0em"] |>
  _[, lon := lon + 12] |>
  _[, time := update(time, hour = 0)] |>
  _[, time := as.Date(time)]
```



```{r rmse_lon-gdata}

rmse_lon_mean_gdata <- rmse_lon_mean |>
  _[version != "persistence"] |>
  _[, lag := as.numeric(as.Date(time) - time_forecast)] |>
  clim_std_lon[i = _, on = .NATURAL] |>
  _[,
    c(
      average(value, climatology),
      list(
        skill = 1 -
          (mean(value, na.rm = TRUE) / mean(climatology, na.rm = TRUE))
      )
    ),
    by = .(lag, lon, version, obs_dataset, month = factor(month(time_forecast)))
  ]


rmse_lon_mean_gdata_dif <- rmse_lon_mean |>
  _[version != "persistence"] |>
  _[, lag := as.numeric(as.Date(time) - time_forecast)] |>
  _[lag <= 217] |>
  dcast(
    lag + lon + obs_dataset + time_forecast + time ~ version,
    value.var = "value"
  ) |>
  clim_std_lon[i = _, on = .NATURAL] |>
  _[,
    c(
      average(S2 - S1),
      list(
        clim = mean(climatology, na.rm = TRUE)
      )
    ),
    by = .(lag, lon, obs_dataset, month = factor(month(time_forecast)))
  ] |>
  _[, skill := estimate / clim]
```

  
```{r, fig.height=7, fig.width=10}
#| label: rmse_lon_plots
sqrt_step <- function(step = 0, root = 2, squish = 1) {
  transform = function(x) {
    ifelse(x >= step, (x - step)^(1 / root) + step, x / squish)
  }
  inverse = function(x) {
    ifelse(x >= sqrt(step), (x - step)^(root) + step, x * squish)
  }
  scales::new_transform("sqrt_step", transform = transform, inverse = inverse)
}
rmse_lon_mean_gdata2 <- rmse_lon_mean_gdata[obs_dataset == which_dataset]

range <- range(rmse_lon_mean_gdata2$skill, na.rm = TRUE)
binwidth <- 0.1
breaks <- AnchorBreaks(0, binwidth = binwidth)(range)
rmse_lon_plots <- lapply(c("S1", "S2", "diff"), \(x) {
  if (x == "diff") {
    rmse_lon_mean_gdata2 <- rmse_lon_mean_gdata_dif |>
      _[obs_dataset == which_dataset] |>
      na.omit()
    breaks <- NULL
  } else {
    rmse_lon_mean_gdata2 <- rmse_lon_mean_gdata2 |>
      _[version == x]
  }
  geom_p <- \() {
    rmse_lon_mean_gdata2 |>
      _[,
        spline(lon, p.value) |> setNames(c("lon", "p.value")),
        by = .(lag, month)
      ] |>
      _[,
        approx(lon, p.value, xout = seq(0, 360, by = 12)) |>
          setNames(c("lon", "p.value")),
        by = .(lag, month)
      ] |>
      _[, p.value := frollmean(p.value, n = 11), by = .(month, lon)] |>
      geom_contour_pval(data = _, aes(z = p.value), p.value = 0.05)
  }
  rmse_lon_mean_gdata2 <- rbind(
    rmse_lon_mean_gdata2,
    rmse_lon_mean_gdata2 |>
      copy() |>
      _[, lon := lon - 360],

    rmse_lon_mean_gdata2 |>
      copy() |>
      _[, lon := lon + 360]
  )

  rmse_lon_mean_gdata2 |>
    _[,
      spline(lon, skill) |> setNames(c("lon", "skill")),
      by = .(lag, month)
    ] |>
    _[,
      approx(lon, skill, xout = seq(0, 360, by = 12)) |>
        setNames(c("lon", "skill")),
      by = .(lag, month)
    ] |>
    _[, skill := frollmean(skill, n = 11), by = .(month, lon)] |>

    ggplot(aes(lon, lag)) +
    geom_contour_fill(
      aes(z = skill, fill = after_stat(level)),
      breaks = AnchorBreaks(0)
    ) +
    geom_contour_tanaka(
      aes(z = skill),
      range = c(0.005, 0.05),
      breaks = AnchorBreaks(0)
    ) +
    geom_p() +
    geom_contour_fill(
      data = topo_lonlat,
      aes(lon, scales::rescale(lat, c(7.5, 10.5)), z = z, fill = NULL),
      fill = "#FAFAFA",
      breaks = c(0, Inf)
    ) +

    geom_contour2(
      data = topo_lonlat,
      aes(lon, scales::rescale(lat, c(7.5, 10.5)), z = z),
      linewidth = 0.2,
      breaks = 0
    ) +

    geom_segment(
      data = month_from_lag(1, 1),
      aes(y = lag, yend = lag, x = 0, xend = 360),
      colour = "gray50",
      alpha = 0.5,
      linewidth = 0.2
    ) +

    annotate(
      "segment",
      x = seq(0, 360, by = 90),
      xend = seq(0, 360, by = 90),
      y = 11,
      yend = max(rmse_lon_mean_gdata2$lag),
      colour = "gray50",
      alpha = 0.5,
      linewidth = 0.2
    ) +

    annotate(
      "segment",
      x = seq(45, 360 - 45, by = 90),
      xend = seq(45, 360 - 45, by = 90),
      y = 11,
      yend = max(rmse_lon_mean_gdata2$lag),
      colour = "gray50",
      alpha = 0.3,
      linewidth = 0.1
    ) +

    # geom_vline(xintercept = ConvertLongitude(c(-60, -30, 70, 150, -150))) +
    geom_text(
      data = month_from_lag(15, 1),
      aes(x = 360, label = month2),
      size = 1.5,
      hjust = -0.2
    ) +

    geom_text(
      data = month_from_lag(1, 1),
      aes(x = 0, label = lag),
      size = 1.5,
      hjust = 1.2
    ) +
    scale_fill_divergent_discretised(
      high = RColorBrewer::brewer.pal(9, "RdBu")[1],
      low = rev(RColorBrewer::brewer.pal(9, "RdBu"))[1],
      mid = "#FAFAFA",
    ) +
    scale_x_longitude(ticks = 90, expand = c(0.07, 0)) +
    scale_y_continuous(
      transform = sqrt_step(step = 11, root = 1.5),
      breaks = NULL,
      expand = expansion(mult = c(0, 0.02))
    ) +
    labs(fill = NULL, y = "Lead time [days]") +
    wide_legend +
    facet_wrap(~month, labeller = labeller(month = labels_month), ncol = 3) +
    theme(
      panel.background = element_blank(),
      panel.grid = element_blank(),
      tagger.panel.tag.background = element_rect(
        colour = "white",
        fill = "white"
      )
    ) +
    tag_facets(position = "bl")
})

```


```{r}
#| label: fig-rmse_lon-1
#| fig-cap: MSE skill score of +S1 forecasts with climatological forecast as reference computed on {uniqueN(rmse_lon_mean$lon)} meridional slices {diff(unique(rmse_lon_mean$lon))[1]}° wide as a function of lead time and longitude. Regions where the difference of means between +S1 RMSE and climatological RMSE are significant at p-value < 0.05 are stipppled. Antarctica’s coastline is shown at the bottom of each panel for reference. Values were smoothed with an 11-day running mean to improve readability. Note that the vertical axis uses a 1.5 root transformation to expand the shorter lead times.
#| fig-height: 7
#| dev: png
rmse_lon_plots[[1]]

```



```{r}
#| label: fig-rmse_lon-2
#| fig-cap: Same as @fig-rmse_lon-1 but for +S2.
#| fig-height: 7
#| dev: png
rmse_lon_plots[[2]]

```

To analyse the spatial distribution of the model error, we computed the RMSE of zonal mean sea-ice concentration anomalies on `r uniqueN(rmse_lon_mean$lon)` slices of `r diff(unique(rmse_lon_mean$lon))[1]`° longitude span for each forecasting system.
We control for some areas being naturally easier to forecast than others by computing the RMSE skill score with the climatological forecast RMSE as reference. 

For +S1 forecasts (@fig-rmse_lon-1 and @fig-rmse_lon-1-osi), skill tends to be lower off the coast of Eastern Antarctica even at short lead times. 
This is particularly important for forecasts initialised between May and July, where we see that the low skill in early Winter (@fig-rmse and @fig-lead-time-window) is only a feature in Eastern Antarctica where skill is evern negative at almost zero lead time. 
+S1 forecats have relatively high predictive skill in the Weddell Sea in winter, consistent with other seasonal prediction systems [@bushuk2021].
July forecasts have positive (albeit not statistically significant) skill with hindcasts initialised as early as February. 
The winter predictive skill barrier thus seems to come in great part from large errors in East Antarctica. 

In West Antarctica there is a hint of easterly-propagating skill in forecasts initialised in February and March. 
This is consistent with @holland2013 findings that memory of sea-ice anomalies are stored in ocean heat content anomalies that are transported east by the Antarctic Circumpolar Current. 

+S2 forecasts  (@fig-rmse_lon-2 and @fig-rmse_lon-2-osi) also have lower skill over East Antarctica. 
From July to December even though the pan-Antarctic average skill is negative at all lead times ([Fig. @fig-lead-time-window]), it is positive (but not statistically significnat) for up to a month in West Antarctic. 
Since oceanic and atmospheric forcing is the only source of information, this suggests that sea-ice in this region is particularly sensitive to oceanic and atmospheric forcing and suggests a role of the Pacific-South American mode and the Amundsen Sea Low to shape sea-ice concentration anomalies. 
The fact that this is evident in the months in which El Niño--Southern Oscillation teleconnections are more important for atmospheric circulation also suggests the influence of tropical Pacific variability.
February and March are the only two months that can be forecasted with marginally positive skill in large regions. 

```{r}
#| label: fig-rmse_lon-3
#| fig-cap: Same as @fig-rmse_lon-1 but for the difference between +S1 and +S2.
#| fig-height: 7
#| dev: png
rmse_lon_plots[[3]]
```

Finally, @fig-rmse_lon-3 (@fig-rmse_lon-3-osi) shows the difference in skill between +S1 and +S2. 
Large differences in skill indicate areas and months that are most affected by the data assimilation present in +S1.
Between January and March, which are the months in which +S1 is the most skillful (@fig-lead-time-window), most of the improvement compared with +S2 is present in the Ross and Weddell Sea. 
In April and May, the improvement seems more homogeneous. 
In June and July, +S1 has higher skill only at very short lead times and in limited regions; in particular we see that in June there is no improvement between 0° and 90°.

## Limitations

This analysis rests on the assumption that the main difference between +S1 and +S2 is the lack of data assimilation of sea-ice concentrations. 
However, although both systems use the same model configuration and atmospheric initial conditions, they also differ in the ensamble generation scheme and their ocean initial conditions.

```{r errors_ensemble} 
errors_ensemble <- here::here("data/derived/rmse.Rds") |>
  zenodo() |>
  readRDS() |>
  overlap_range(time_forecast) |>
  _[version != "persistence"] |>
  _[member != "0em"] |>
  _[value < 0.01, value := NA] |>
  _[value > 3e13, value := NA] |>
  _[!is.na(value)] |>
  _[, time := as.Date(time)] |>
  _[, lag := as.numeric(time - time_forecast)] |>
  _[lag > 0] |>
  _[!(month(time_forecast) == 1 & member == "07" & version == "S1")]

``` 

```{r fig-spread}
#| fig-cap: Mean variance of RMSE between ensemble members of each forecast. Note the double log scale. In parentheses, the shortest minimum  lead time at which +S1 mean spread becomes larger than the lower bound of the 95% confidence interval of +S2 spread.
#| fig-height: 4
spread <- errors_ensemble |>
  _[measure == "rmse"] |>
  _[obs_dataset == which_dataset] |>
  _[, var(value, na.rm = TRUE), by = .(version, lag, time_forecast)] |>
  _[,
    average(V1),
    by = .(version, month(time_forecast), lag)
  ]

min_lag_equal <- spread |>
  copy() |>
  _[, base := low[version == "S2"], by = .(lag, month)] |>
  _[version != "S2"] |>
  _[estimate > base] |>
  _[, .SD[which.min(lag)], by = .(month)] |>
  _[order(month)]


labels_month_significant <- setNames(
  paste0(month.abb, " (", min_lag_equal$lag, ")"),
  1:12
)

spread |>
  ggplot(aes(lag, estimate)) +
  geom_ribbon(
    aes(ymin = low, ymax = high, fill = version),
    alpha = 0.5
  ) +
  geom_line(aes(color = version)) +
  facet_wrap(~month, labeller = labeller(month = labels_month_significant)) +
  scale_y_log10("Mean ensemble spread", guide = guide_axis_logticks()) +
  scale_x_log10("Lead time [days]", guide = guide_axis_logticks()) +
  scale_color_models() +
  scale_fill_models()
```

As mentioned in @sec-systems, +S1 ensemble members are generated by adding random field perturbations to the atmosphere only, which then are transferred to the other components via the coupled simulation [@hudson2017] while +S2 ensemble members are generated by time-lagged ensemble. 

We believe this difference would only alter the ensemble spread and should have minimal impact in the dramatic differences in ensemble mean error shown in @fig-rmse. 
In +S1's scheme, ensemble members are all but guaranteed to be underdispersed in the ocean and sea-ice components. 
To test the effect of the ensemble generation scheme on ensemble spread, we compute the mean variance of RMSE of individual forecasts as a function of lag for each forecasting system. 
@fig-spread (@fig-spread-osi) shows that indeed +S1's initial spread is much lower than +S2's in every month. 
However, +S1's ensemble spread grows quickly, and after less than a week, both systems's spread is comparable. 
For forecasts initialised in February, March and April, +S1 even has larger spread than +S2. 

Regarding ocean initial conditions, unfortunately we cannot directly compare +S1 and +S2 initial conditions because that data is currently not available. 
However, we believe there are reasons to consider that the effect would be of second order importance. 

While the ocean data assimilation process is different, the underlying observations are similar, especially in the high latitudes, where there are few direct observations in total. 
As an indirect comparison of ocean initial conditions, @wedd2022 in their Figure 9 show the correlation of  ocean heat content in the upper 300m of the ocean between EN4 and +S2 and +S1 reanalysis. 
In both systems, correlations are raltively high around 60° and very low in the Antarctic coast; importantly, their correlation patterns are very similar, suggesting that the initial conditions sare not too dissimilar between the two systems. 
A limitation of this comparison is that the low correlations seen in both systems in the ice-covered regions only show that both systems have poor ocean initial conditions in that region but does not necessarily mean that their initial conditions are similar. 
Even though the ocean initial conditions are similarly correlated with the reference dataset in both systems, SST forecast was shown by @wedd2022 to be extremely degraded in the Southern Ocean even at zero months lead time. 
Correlation of SST anomalies in the ice-covered regions is much lower for +S2 (@wedd2022 Figure 13) and with a higher positive bias (their Figure 12), which is consistent with the greater negative sea-ice concentration bias in +S2 (@fig-hindcast-extent). 
This is raises the possibility that it is the lack of assimilation of sea-ice data that is degradidng SST forecast and not the other way around. 


\FloatBarrier

# Conclusions

Sea-ice forecasts from the +S2 system show a significant low extent bias, particularly during late summer and early autumn. 
This bias is attributed to a faster and longer melt season between January and March, and slower growth between March and April. 
This underestimation during the minimum and early freezing season is a common issue in many subseasonal-to-seasonal (S2S) systems, suggesting potential problems either with the model’s thermodynamic representation or with short wave radiation forcing, as shown in other climate models [@zampieri2019; @roach2020].
Even though +S2 shares the same model components as +S1, the latter does not suffer from this bias, indicating that assimilating sea-ice concentrations successfully corrects for the negative bias that exists in the free-running model.

Ensemble spread grows quickly even when perturbations are only implemented in the atmosphere component (in +S1), indicating that sea ice is indeed responding quickly to atmospheric perturbations. 
However, our analysis suggests that the atmosphere and ocean data assimilation implemented in +S2 is only effectively influencing sea-ice initial conditions from June to October, while the rest of the year, the sea-ice component runs virtually free, reverting to its biased equilibrium state. 
@zhou2022 had previously evaluated sea-ice forecasts in +S2 and also highlighted the poor performance of this forecasting system attributed to the lack of good initial conditions.

Although +S1 only assimilates sea-ice concentration, it is clear that sea-ice thickness is also affected through the assimilation process. 
+S1 simulates significantly thicker ice than +S2 and in both systems sea-ice is thicker at shorter lead times than at longer lead times. 
Both the explicit data assimilation in +S2 and the effects of atmospheric and oceanic data assimilation in +S1 might be nudging simulated sea ice to be thicker than the model equilibrium state.
It still remains unclear why +S2 simulates much larger interannual variability than +S1 at shorter leadtimes. 
We suggest that the thinner sea ice in +S2 contributes to the large sea-ice extent variance, but other mechanisms, such as unbalanced initial conditions might also be important. 

Given that +S2 sea-ice extent is not directly initialised by sea-ice observations, comparing its forecasts with those of +S1 allows us to estimate the time-scale over which initial conditions are important. 
We find that initial conditions affect Antarctic sea-ice forecasts in the order of a few months, but that effect is seasonally dependent. 
January to April initial conditions improve forecasts for up to three months.
February initial conditions in particular are shown to be crucial for determining sea-ice evolution at least up to May. 
Arctic sea-ice forecasts also show greater sensitivity to initial conditions in boreal summer, compared with boreal winter [@day2014; @bunzel2016], suggesting a similar mechanism might be playing a role. 

These results contrasts with @xiu2025, which used a very similar methodology to our own.   
Based on their claims that assimilation of sea-ice concentration data doesn't improve sea-ice forecasts compared with only assimilating ocean data and that atmosphere conditions are enough for a skillful sea-ice forecast we would have expected skillful forecast from +S2's atmosphere initial conditions. 
It is possible that model biases are important to explain this discrepancy. 
Their forecasting system simulated more sea ice than observations; the reverse of +S1 and +S2, which tended to drift into lower sea-ice concentration that needed to be corrected with data assimilation. 
However, since they used anomaly field assimilation, which assimilates anomalies on top of the model's own climatology, their initial conditions were close to the model's own equilibrium state, reducing the strong drift the type we detected in +S1 and we speculate might be degrading +S2's variability. 

Forecasts initialised in the winter have very little skill and +S1 and +S2 forecast errors are statistically indistinguishable after just two weeks. 
Contra @libera2022 but consisitent with @xiu2025, we find that this predictive skill barrier is not apparent in the Weddell Sea and instead comes from low skill in the Indian Ocean sector. 
However, we cannot make a direct comparison due to significant methodological differences.
Their results were based on persistence of sea-ice area, whereas we study errors in sea-ice concentration anomalies of a dynamical model. 
Further exploration of these differences could lead to more insight in how persistence-based measures of predictability translate to actual predictive skill. 

These findings have important implications for both operational forecasting, model development and predictability studies.
For operational centers, our results suggest that efforts to improve sea-ice data assimilation should prioritize the summer and autumn months when initial conditions have the greatest impact on forecast skill. 
Additionally, the substantial bias in +S2 highlights the need for improved model physics, particularly in the representation of sea-ice thermodynamics and radiation processes. 
Crucially, our results suggest dramatic seasonal variations in sea-ice predictability. 
Our conclusions are limited by the fact that +S1 and +S2 differ in the ocean data assimilation and also the ensemble generation method. 
Although we believe these differences to be of minor importance compared with the wildly divergent sea-ice initial conditions, they motivate the need for targeted experiments. 
Future studies should therefore use initial conditions through the whole year rather than focusing on a limited number of initialisation dates. 


# References

::: {#refs}
:::


\clearpage



# Appendix {.appendix}

\appendixfigures

The following are the same figures from the main paper but using the OSI dataset instead of CDR.

```{r dataset_era}
which_dataset <- "osi"
```

```{r fig-hindcast-extent-osi, ref.label=I('fig-hindcast-extent')}

```

\clearpage

```{r, fig-mean-growth-osi, ref.label=I("fig-mean-growth")}

```

\clearpage

```{r, plot_bias-osi, ref.label=I("plot_bias")}

```

\clearpage

```{r, fig-bias-1-osi, ref.label=I("fig-bias-1")}

```

\clearpage

```{r, fig-bias-2-osi, ref.label=I("fig-bias-2")}

```

\clearpage

```{r, fig-extent-anom-osi, ref.label=I("fig-extent-anom")}

```

\clearpage

```{r fig-extent-rmse-month}
#| fig-cap: RMSE of monthly mean sea-ice extent anomalies as a function of lead time (months) for +S1 (green) and +S2 (purple). RMSE is computed over the overlapping period of +S1 and +S2 hindcasts (1990–2013). Each panel indicates the target month. Note the reverse horizontal axis.
#| fig-height: 6
fig_extent_rmse_month
```


\clearpage


```{r, fig-extent-sd-osi, ref.label=I("fig-extent-sd")}
```


\clearpage


```{r}
#| label: cor_hi
mean_aice <- S2_reanalysis() |>
  cdo_ymonmean() |>
  cdo_setyear(2000) |>
  cdo_setday(1) |>
  _[] |>
  ReadNetCDF(c(mean = "aice")) |>
  _[, .(xgrid, ygrid, month = month(time), mean)]


mask <- S2_reanalysis("hi") |>
  cdo_ymonmean() |>
  cdo_setyear(2000) |>
  cdo_setday(1) |>
  _[] |>
  ReadNetCDF(c(mean = "hi")) |>
  _[, .(xgrid, ygrid, month = month(time), mean)]


cor_hi <- lapply(1:12, \(month) {
  merge(
    S2_reanalysis("hi") |>
      cdo_selmonth(month) |>
      cdo_monmean() |>
      _[] |>
      ReadNetCDF("hi") |>
      na.omit(),

    S2_reanalysis() |>
      anomalies() |>
      cdo_selmonth(month) |>
      cdo_monmean() |>
      cdo_sqr() |>
      _[] |>
      ReadNetCDF("aice") |>
      na.omit()
  ) |>
    _[, correlate(hi, aice), by = .(xgrid, ygrid, month(time))] |>
    na.omit()
})
```

```{r}
#| label: fig-hi-var-cor
#| fig-cap: Correlation between monthly mean sea-ice thickness and squared monthly mean sea-ice concentration anomalies masked to areas with mean thinkes greater than 15cm (black contour) in the +S2 reanalysis. Regions where the correlation is significant at p-value < 0.05 are stipppled. In most regions and months, thicker ice is correlated with reduced sea-ice concentration anomalies.
#| fig-height: 6

cor_hi |>
  rbindlist() |>
  mask[i = _, on = .NATURAL] |>
  _[mean > 0.15] |>
  ggplot(aes(xgrid, ygrid)) +
  geom_contour_fill(
    aes(z = estimate, fill = ..level..),
    breaks = AnchorBreaks(exclude = 0)
  ) +
  scale_fill_divergent_discretised(
    NULL,
    high = RColorBrewer::brewer.pal(9, "RdBu")[1],
    low = rev(RColorBrewer::brewer.pal(9, "RdBu"))[1]
  ) +
  geom_contour_pval(aes(z = p.value)) +
  geom_contour2(data = mask, aes(z = mean), breaks = 0.15, linewidth = 0.2) +
  wide_legend +
  facet_wrap(~month, labeller = labeller(month = setNames(month.abb, 1:12))) +
  geom_antarctica_fill +
  geomcoord_antarctica +
  scale_y_continuous(NULL, breaks = NULL, expand = c(0, 0)) +
  scale_x_continuous(NULL, breaks = NULL, expand = c(0, 0)) +
  geom_coords() +
  ggplot2::coord_sf(
    xlim = box[1:2],
    ylim = box[3:4],
    crs = sic_projection,
    lims_method = "box",
    label_axes = "----"
  )

```

\clearpage

```{r, fig-rmse-osi, ref.label=I("fig-rmse")}

```

\clearpage



```{r fig-lead-time-window}
#| fig-cap: Minimum lead time at which each forecast's mean RMSE becomes lower than the lower bound of the 95% confidence interval of persistence forecast RMSE (black lines) and maximum lead time at which each forecast's mean RMSE remains lower than the lower bound of the 95% confidence interval of climatological forecast RMSE (gray lines). Green shading indicates the window where forecasts outperform both persistence (lead times longer than black line) and climatology (lead times shorter than gray line). Text labels show the date corresponding to the maximum lead time at which each forecast outperforms climatology. Using CDR as the validation dataset.
#| fig-height: 4

rbind(
  errors2 |>
    copy() |>
    _[obs_dataset == which_dataset] |>
    _[,
      base := low[version == "persistence"],
      by = .(lag, measure, month, obs_dataset)
    ] |>
    _[version != "persistence"] |>
    _[version != "climatology"] |>
    _[estimate < base] |>
    _[, .SD[which.min(lag)], by = .(version, measure, month, obs_dataset)] |>
    _[, .(version, month, obs_dataset, lag, benchmark = "persistence")],

  errors2 |>
    copy() |>
    _[obs_dataset == which_dataset] |>
    _[,
      base := low[version == "climatology"],
      by = .(lag, measure, month, obs_dataset)
    ] |>
    _[version != "climatology"] |>
    _[version != "persistence"] |>
    # _[lag > 2] |>
    _[, rle(estimate < base), by = .(version, measure, month, obs_dataset)] |>
    _[, .SD[1], by = .(version, measure, month, obs_dataset)] |>
    tidyr::complete(
      version,
      measure,
      month,
      obs_dataset,
      values,
      fill = list(lengths = 0)
    ) |>
    as.data.table() |>
    _[values == TRUE] |>
    _[, .(
      version,
      month,
      obs_dataset,
      lag = lengths,
      benchmark = "climatology"
    )]
) |>
  _[, time := make_date(2000, month, 1) + lag] |>
  # _[benchmark == "climatology" & version == "S1"] |>
  ggplot(aes(month, lag)) +
  ggbraid::geom_braid(
    data = \(x) dcast(x, month + lag + version ~ benchmark, value.var = "lag"),
    alpha = 0.3,
    aes(
      ymin = persistence,
      ymax = climatology,
      fill = climatology > persistence
    )
  ) +
  geom_line(aes(color = benchmark)) +
  # geom_point(aes(color = time), data = \(x) x[lag != 0]) +
  ggrepel::geom_text_repel(
    data = \(x) {
      x[, .SD[which.max(lag)], by = .(version, month, obs_dataset)] |>
        _[benchmark == "climatology"]
    },
    aes(label = format(time, "%d %b")),
    size = 2,
    vjust = -0.4
  ) +
  geom_point(
    data = \(x) {
      x[, .SD[which.max(lag)], by = .(version, month, obs_dataset)] |>
        _[benchmark == "climatology"]
    },
    aes(color = benchmark)
  ) +
  # geom_line(data = dif[obs_dataset == which_dataset]) +
  scale_color_models() +
  scale_y_continuous("Lead time [days]") +
  scale_x_continuous(
    "Forecast month",
    breaks = 1:12,
    labels = month.abb,
    minor_breaks = NULL
  ) +
  scale_fill_manual(
    guide = "none",
    values = c(`TRUE` = "seagreen", `FALSE` = "#FFFFFF00")
  ) +
  facet_wrap(~version, ncol = 1, labeller = labeller(version = labels_models)) +
  coord_cartesian(clip = "off") +
  tag_facets()

```

\clearpage

```{r, fig-lead-time-window-osi, ref.label=I("fig-lead-time-window")}
#| fig-cap: Same as @fig-lead-time-window but using OSI as the validation dataset.

```

\clearpage

```{r, rmse_lon_plots-osi, ref.label=I("rmse_lon_plots")}

```

\clearpage

```{r, fig-rmse_lon-1-osi, ref.label=I("fig-rmse_lon-1")}

```

\clearpage

```{r, fig-rmse_lon-2-osi, ref.label=I("fig-rmse_lon-2")}

```

\clearpage

```{r, fig-rmse_lon-3-osi, ref.label=I("fig-rmse_lon-3")}

```

\clearpage

```{r, fig-spread-osi, ref.label=I("fig-spread")}

```
